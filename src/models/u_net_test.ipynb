{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, Reshape, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.linalg import sqrtm\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "\n",
    "import cv2 # Import OpenCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "from sklearn.utils import check_random_state\n",
    "check_random_state(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NET</th>\n",
       "      <th>Cooling</th>\n",
       "      <th>PV_meter1_load (kW)</th>\n",
       "      <th>PV_meter2_load (kW)</th>\n",
       "      <th>Battery cabinet</th>\n",
       "      <th>Sumppump</th>\n",
       "      <th>Plug_Basement</th>\n",
       "      <th>Solar rapid shutdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.422260</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>-0.050127</td>\n",
       "      <td>-0.048380</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.190117</td>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.438863</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>-0.049362</td>\n",
       "      <td>-0.050536</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.198712</td>\n",
       "      <td>0.004033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.401933</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>-0.048622</td>\n",
       "      <td>-0.047862</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.163461</td>\n",
       "      <td>0.004018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.410452</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>-0.048963</td>\n",
       "      <td>-0.050321</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.173353</td>\n",
       "      <td>0.003993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.426215</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>-0.048785</td>\n",
       "      <td>-0.049283</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.197125</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NET   Cooling  PV_meter1_load (kW)  PV_meter2_load (kW)  \\\n",
       "0  1.422260  0.002001            -0.050127            -0.048380   \n",
       "1  1.438863  0.002010            -0.049362            -0.050536   \n",
       "2  1.401933  0.001990            -0.048622            -0.047862   \n",
       "3  1.410452  0.002003            -0.048963            -0.050321   \n",
       "4  1.426215  0.001997            -0.048785            -0.049283   \n",
       "\n",
       "   Battery cabinet  Sumppump  Plug_Basement  Solar rapid shutdown  \n",
       "0         0.003126  0.001130       0.190117              0.003929  \n",
       "1         0.003116  0.001148       0.198712              0.004033  \n",
       "2         0.003119  0.001133       0.163461              0.004018  \n",
       "3         0.003120  0.001137       0.173353              0.003993  \n",
       "4         0.003112  0.001126       0.197125              0.003896  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meter_data_dir = \"../preprocessing/data/meter_data/48190963_Loads_hourly.csv\"\n",
    "conditioning_data_dir = \"../preprocessing/data/conditioning_data/48190936_Weather.csv\"\n",
    "\n",
    "meter_data = pd.read_csv(meter_data_dir)\n",
    "meter_data = meter_data[[\"NET\", \"Cooling\", \"PV_meter1_load (kW)\", \"PV_meter2_load (kW)\", \"Battery cabinet\", \"Sumppump\", \"Plug_Basement\", \"Solar rapid shutdown\"]] #subset for dev\n",
    "#meter_data = meter_data[[\"NET\", \"Cooling\", \"PV_meter1_load (kW)\", \"Battery cabinet\"]] #subset for dev\n",
    "print(meter_data.shape)\n",
    "meter_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air temperature (gund, ¡É)</th>\n",
       "      <th>Relative humidity (gund, %)</th>\n",
       "      <th>Wind speed (gund, m/s)</th>\n",
       "      <th>Weather data, Wind speed (m/s)</th>\n",
       "      <th>Wind direction (gund, ¡Æ)</th>\n",
       "      <th>Weather data, Wind direction</th>\n",
       "      <th>Weather data, Rain</th>\n",
       "      <th>Solar radiation (gund, W/m^2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.576667</td>\n",
       "      <td>88.250000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>98.583333</td>\n",
       "      <td>129.857023</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.975833</td>\n",
       "      <td>89.908333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>111.083333</td>\n",
       "      <td>129.857023</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.330833</td>\n",
       "      <td>90.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>98.333333</td>\n",
       "      <td>129.857023</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.648333</td>\n",
       "      <td>87.258333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>84.166667</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.836667</td>\n",
       "      <td>80.758333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>85.833333</td>\n",
       "      <td>119.250157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Air temperature (gund, ¡É)  Relative humidity (gund, %)  \\\n",
       "0                   12.576667                    88.250000   \n",
       "1                   12.975833                    89.908333   \n",
       "2                   13.330833                    90.491667   \n",
       "3                   13.648333                    87.258333   \n",
       "4                   13.836667                    80.758333   \n",
       "\n",
       "   Wind speed (gund, m/s)  Weather data, Wind speed (m/s)  \\\n",
       "0                0.208333                        0.033333   \n",
       "1                0.000000                        0.033333   \n",
       "2                0.000000                        0.033333   \n",
       "3                0.250000                        0.050000   \n",
       "4                0.166667                        0.050000   \n",
       "\n",
       "   Wind direction (gund, ¡Æ)  Weather data, Wind direction  \\\n",
       "0                  98.583333                    129.857023   \n",
       "1                 111.083333                    129.857023   \n",
       "2                  98.333333                    129.857023   \n",
       "3                  84.166667                    109.500000   \n",
       "4                  85.833333                    119.250157   \n",
       "\n",
       "   Weather data, Rain  Solar radiation (gund, W/m^2)  \n",
       "0                   0                            1.0  \n",
       "1                   0                            1.0  \n",
       "2                   0                            1.0  \n",
       "3                   0                            1.0  \n",
       "4                   0                            1.0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditioning_data = pd.read_csv(conditioning_data_dir, encoding='unicode_escape')\n",
    "conditioning_data = conditioning_data.iloc[:, 1:]\n",
    "print(conditioning_data.shape)\n",
    "conditioning_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 8, 8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_var_ts = np.asarray(meter_data)\n",
    "gaf = GramianAngularField(image_size=multi_var_ts.shape[1], method='summation')\n",
    "gaf_images = [gaf.fit_transform(ts.reshape(1, -1))[0] for ts in multi_var_ts]\n",
    "ts_images = np.asarray(gaf_images)\n",
    "ts_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-net with n-downsampling layers, input needs to be divisible by 2^n<br>\n",
    "i.e. 4 downsampling layers = 2^4 = 16x16 img shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = ts_images.shape[1]\n",
    "img_cols = ts_images.shape[2]\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 1, 8, 8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.tile(ts_images, (1, 1, 1, channels))\n",
    "X_train = np.transpose(X_train, (1, 0, 2, 3))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.asarray(conditioning_data)\n",
    "dims_meta = y_train.shape[1]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "# Generate 10 unique random indices\n",
    "random_indices = random.sample(range(X_train.shape[0]), 10)\n",
    "\n",
    "# Extract the randomly selected images\n",
    "random_images = X_train[random_indices]\n",
    "\n",
    "# Now random_images has shape [10, 1, 64, 64]\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.imshow(random_images[idx].squeeze(), cmap='gray') # squeeze to remove the channel dimension\n",
    "    ax.axis('off') # to remove the axes for clarity\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if input and output channels are the same for the residual connection\n",
    "        self.same_channels = in_channels == out_channels\n",
    "\n",
    "        # Flag for whether or not to use residual connection\n",
    "        self.is_res = is_res\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n",
    "            nn.BatchNorm2d(out_channels),   # Batch normalization\n",
    "            nn.GELU(),   # GELU activation function\n",
    "        )\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n",
    "            nn.BatchNorm2d(out_channels),   # Batch normalization\n",
    "            nn.GELU(),   # GELU activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # If using residual connection\n",
    "        if self.is_res:\n",
    "            # Apply first convolutional layer\n",
    "            x1 = self.conv1(x)\n",
    "\n",
    "            # Apply second convolutional layer\n",
    "            x2 = self.conv2(x1)\n",
    "\n",
    "            # If input and output channels are the same, add residual connection directly\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                # If not, apply a 1x1 convolutional layer to match dimensions before adding residual connection\n",
    "                shortcut = nn.Conv2d(x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0).to(x.device)\n",
    "                out = shortcut(x) + x2\n",
    "            #print(f\"resconv forward: x {x.shape}, x1 {x1.shape}, x2 {x2.shape}, out {out.shape}\")\n",
    "\n",
    "            # Normalize output tensor\n",
    "            return out / 1.414\n",
    "\n",
    "        # If not using residual connection, return output of second convolutional layer\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "    # Method to get the number of output channels for this block\n",
    "    def get_out_channels(self):\n",
    "        return self.conv2[0].out_channels\n",
    "\n",
    "    # Method to set the number of output channels for this block\n",
    "    def set_out_channels(self, out_channels):\n",
    "        self.conv1[0].out_channels = out_channels\n",
    "        self.conv2[0].in_channels = out_channels\n",
    "        self.conv2[0].out_channels = out_channels\n",
    "        \n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        \n",
    "        # Create a list of layers for the upsampling block\n",
    "        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        \n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        \n",
    "        # Pass the concatenated tensor through the sequential model and return the output\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        \n",
    "        # Create a list of layers for the downsampling block\n",
    "        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), ResidualConvBlock(out_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        \n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the sequential model and return the output\n",
    "        return self.model(x)\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        This class defines a generic one layer feed-forward neural network for embedding input data of\n",
    "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # define the layers for the network\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        \n",
    "        # create a PyTorch sequential model consisting of the defined layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input tensor\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        # apply the model layers to the flattened tensor\n",
    "        return self.model(x)\n",
    "    \n",
    "def unorm(x):\n",
    "    # unity norm. results in range of [0,1]\n",
    "    # assume x (h,w,3)\n",
    "    xmax = x.max((0,1))\n",
    "    xmin = x.min((0,1))\n",
    "    return(x - xmin)/(xmax - xmin)\n",
    "\n",
    "def norm_all(store, n_t, n_s):\n",
    "    # runs unity norm on all timesteps of all samples\n",
    "    nstore = np.zeros_like(store)\n",
    "    for t in range(n_t):\n",
    "        for s in range(n_s):\n",
    "            nstore[t,s] = unorm(store[t,s])\n",
    "    return nstore\n",
    "\n",
    "def norm_torch(x_all):\n",
    "    # runs unity norm on all timesteps of all samples\n",
    "    # input is (n_samples, 3,h,w), the torch image format\n",
    "    x = x_all.cpu().numpy()\n",
    "    xmax = x.max((2,3))\n",
    "    xmin = x.min((2,3))\n",
    "    xmax = np.expand_dims(xmax,(2,3)) \n",
    "    xmin = np.expand_dims(xmin,(2,3))\n",
    "    nstore = (x - xmin)/(xmax - xmin)\n",
    "    return torch.from_numpy(nstore)\n",
    "\n",
    "def gen_tst_context(n_cfeat):\n",
    "    \"\"\"\n",
    "    Generate test context vectors\n",
    "    \"\"\"\n",
    "    vec = torch.tensor([\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0]]      # human, non-human, food, spell, side-facing\n",
    "    )\n",
    "    return len(vec), vec\n",
    "\n",
    "def plot_grid(x,n_sample,n_rows,save_dir,w):\n",
    "    # x:(n_sample, 3, h, w)\n",
    "    ncols = n_sample//n_rows\n",
    "    grid = make_grid(norm_torch(x), nrow=ncols)  # curiously, nrow is number of columns.. or number of items in the row.\n",
    "    save_image(grid, save_dir + f\"run_image_w{w}.png\")\n",
    "    print('saved image at ' + save_dir + f\"run_image_w{w}.png\")\n",
    "    return grid\n",
    "\n",
    "def plot_sample(x_gen_store,n_sample,nrows,save_dir, fn,  w, save=False):\n",
    "    ncols = n_sample//nrows\n",
    "    sx_gen_store = np.moveaxis(x_gen_store,2,4)                               # change to Numpy image format (h,w,channels) vs (channels,h,w)\n",
    "    nsx_gen_store = norm_all(sx_gen_store, sx_gen_store.shape[0], n_sample)   # unity norm to put in range [0,1] for np.imshow\n",
    "    \n",
    "    # create gif of images evolving over time, based on x_gen_store\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=True,figsize=(ncols,nrows))\n",
    "    def animate_diff(i, store):\n",
    "        print(f'gif animating frame {i} of {store.shape[0]}', end='\\r')\n",
    "        plots = []\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                axs[row, col].clear()\n",
    "                axs[row, col].set_xticks([])\n",
    "                axs[row, col].set_yticks([])\n",
    "                plots.append(axs[row, col].imshow(store[i,(row*ncols)+col]))\n",
    "        return plots\n",
    "    ani = FuncAnimation(fig, animate_diff, fargs=[nsx_gen_store],  interval=200, blit=False, repeat=True, frames=nsx_gen_store.shape[0]) \n",
    "    plt.close()\n",
    "    if save:\n",
    "        ani.save(save_dir + f\"{fn}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "        print('saved gif at ' + save_dir + f\"{fn}_w{w}.gif\")\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=64):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat)\n",
    "\n",
    "        # Only two down-sampling layers\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 4 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((self.h//8)), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 4 * n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 2 * n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 4 * n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 2 * n_feat)\n",
    "\n",
    "        # Only two up-sampling layers\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4 * n_feat, 4 * n_feat, self.h//8, self.h//8),\n",
    "            nn.GroupNorm(8, 4 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(8 * n_feat, 2 * n_feat)\n",
    "        self.up2 = UnetUp(3 * n_feat, n_feat)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        #print(f\"input shape: {x.shape}\")\n",
    "        x = self.init_conv(x)\n",
    "        #print(f\"init conv shape: {x.shape}\")\n",
    "        down1 = self.down1(x)\n",
    "        #print(f\"down1 shape: {down1.shape}\")\n",
    "        down2 = self.down2(down1)\n",
    "        #print(f\"down2 shape: {down2.shape}\")\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        #print(f\"hiddenvec shape: {hiddenvec.shape}\")\n",
    "        \n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x.device)\n",
    "\n",
    "        #TODO if c is not none pas through embedding\n",
    "        \n",
    "        # Embeddings\n",
    "        cemb1 = self.contextembed1(c).view(x.shape[0], self.n_feat * 4, 1, 1)\n",
    "        #print(f\"cemb1 shape: {cemb1.shape}\")\n",
    "        temb1 = self.timeembed1(t).view(x.shape[0], self.n_feat * 4, 1, 1)\n",
    "        #print(f\"temb1 shape: {temb1.shape}\")\n",
    "        cemb2 = self.contextembed2(c).view(x.shape[0], self.n_feat * 2, 1, 1)\n",
    "        #print(f\"cemb2 shape: {cemb2.shape}\")\n",
    "        temb2 = self.timeembed2(t).view(x.shape[0], self.n_feat * 2, 1, 1)\n",
    "        #print(f\"temb2 shape: {temb2.shape}\")\n",
    "\n",
    "        # Upsampling\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        #print(f\"up1 shape: {up1.shape}\")\n",
    "        up2 = self.up1(cemb1 * up1 + temb1, down2)\n",
    "        #print(f\"up2 shape: {up2.shape}\")\n",
    "        up3 = self.up2(cemb2 * up2 + temb2, down1)\n",
    "        #print(f\"up3 shape: {up3.shape}\")\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height: 8\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = y_train.shape[1]\n",
    "height = img_cols# 16x16 image\n",
    "print(f\"height: {height}\")\n",
    "\n",
    "x_tensor = torch.from_numpy(X_train)\n",
    "dataset = TensorDataset(x_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=5)\n",
    "sample = next(iter(dataloader))[0]\n",
    "sample = sample.to(device)\n",
    "\n",
    "t = torch.randint(1, 500 + 1, (sample.shape[0],)).to(device)\n",
    "\n",
    "nn_model = ContextUnet(in_channels=channels, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
    "out = nn_model(sample.float(), t.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = int(y_train.shape[1]) # context vector is of size 7\n",
    "height = img_cols # 16x16 image\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 10 #100\n",
    "n_epoch = 5\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8760, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "x_tensor = torch.from_numpy(X_train)\n",
    "y_tensor = torch.from_numpy(y_train)\n",
    "print(y_tensor.shape)\n",
    "\n",
    "# Create a TensorDataset from the tensors\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = ContextUnet(in_channels=channels, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
    "\n",
    "# re setup optimizer\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, nrow=2):\n",
    "  _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4,2 ))\n",
    "  axs = axs.flatten()\n",
    "\n",
    "  if imgs.shape[1]==1:\n",
    "    imgs = imgs.repeat(1, 3, 1, 1)\n",
    "\n",
    "  for img, ax in zip(imgs, axs):\n",
    "      img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "      ax.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm_context(n_sample, context, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, channels, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t, ctx)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def rmse(a, b):\n",
    "    return torch.sqrt(torch.mean((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 876/876 [00:12<00:00, 71.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 876/876 [00:11<00:00, 73.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 876/876 [00:11<00:00, 75.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 876/876 [00:11<00:00, 76.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 876/876 [00:11<00:00, 75.56it/s]\n"
     ]
    }
   ],
   "source": [
    "nn_model.train()\n",
    "\n",
    "list_fid_score = []\n",
    "list_rmse_score = []\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "\n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    epoch_loss = 0.0   # initialize epoch loss\n",
    "    n_batches = 0      # count number of batches\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        c = c.to(x)\n",
    "        \n",
    "        # randomly mask out c\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
    "        c = c * context_mask.unsqueeze(-1)\n",
    "\n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        noise = noise.type(torch.float32)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "\n",
    "        # use network to recover noise\n",
    "        time = t / timesteps\n",
    "        time = time.type(torch.float32)\n",
    "        x_pert = x_pert.type(torch.float32)\n",
    "        c = c.type(torch.float32)\n",
    "        \n",
    "        pred_noise = nn_model(x_pert, time, c=c)\n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "## No Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep   1\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c576787e80>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_img, _ = sample_ddpm_context(1, None)\n",
    "gen_img = gen_img.cpu().numpy()\n",
    "gen_img = np.transpose(gen_img, (0, 2, 3, 1))\n",
    "gen_img = gen_img.squeeze(-1)\n",
    "plt.imshow(gen_img.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep   1\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c576b3c7f0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cond = torch.tensor(y_train[0]).float().to(device)\n",
    "gen_img, _ = sample_ddpm_context(1, cond)\n",
    "gen_img = gen_img.cpu().numpy()\n",
    "gen_img = np.transpose(gen_img, (0, 2, 3, 1))\n",
    "gen_img = gen_img.squeeze(-1)\n",
    "plt.imshow(gen_img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
