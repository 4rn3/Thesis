{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf9a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from models.transformer_encoder_x_attn import TransEncoder\n",
    "from ddpm.ddpm import GaussianDiffusion1D\n",
    "from evaluation.evaluation import vizual_comparison, plot_jsd_per_customer, plot_kde_samples, make_gif_from_images, mmd_histogram_per_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfccbdb",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebe25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "seq_len = 12\n",
    "batch_size = 256\n",
    "k = 15\n",
    "\n",
    "#NN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "latent_dim = 256\n",
    "cond_model = \"mlp\"\n",
    "num_layers = 6\n",
    "n_heads = 8\n",
    "lr = 1e-4\n",
    "betas = (0.9, 0.99)\n",
    "epochs = 2000\n",
    "save_rate = 100\n",
    "\n",
    "## DDPM\n",
    "timesteps = 1000\n",
    "beta_schedule = \"cosine\"\n",
    "objective = \"pred_noise\"\n",
    "\n",
    "## Logging\n",
    "experiment_name = \"trans_x_attn\"\n",
    "logging_dir = f\"./logging/{experiment_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac50d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "    os.makedirs(os.path.join(logging_dir, \"viz/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"jsd/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"kde/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"tensorboard/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"weights/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"mmd/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fcd1e",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10548a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = \"./preprocessing/data/customer_led_network_revolution/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3ef856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDATA(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        seq_data = []\n",
    "        for i in range(len(data) - seq_len + 1):\n",
    "            x = data[i : i + seq_len]\n",
    "            seq_data.append(x)\n",
    "        self.samples = np.asarray(seq_data, dtype=np.float32) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f250cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    cluster_labels = kmeans.fit_predict(data.T)\n",
    "    \n",
    "    clustered_data = []\n",
    "    for cluster in range(kmeans.n_clusters):\n",
    "        cluster_data = data.iloc[:, cluster_labels == cluster].mean(axis=1)\n",
    "        clustered_data.append(cluster_data)\n",
    "        \n",
    "    return pd.DataFrame(clustered_data).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b79f4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a786e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"test.csv\"))\n",
    "cond_train = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_train.csv\"))\n",
    "cond_val = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_val.csv\"))\n",
    "cond_test = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73f13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(\"Date and Time of capture\", axis=1, inplace=True)\n",
    "val.drop(\"Date and Time of capture\", axis=1, inplace=True)\n",
    "test.drop(\"Date and Time of capture\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d436e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "train = cluster(train, k)\n",
    "test = cluster(test, k)\n",
    "val = cluster(val, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbeb7d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33984, 15), (6796, 15), (1700, 15), (33984, 27), (6796, 27), (1700, 27))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape, cond_train.shape, cond_val.shape, cond_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73b6440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33973, 15, 12), (33973, 27, 12), (1689, 15, 12), (1689, 27, 12))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq = np.asarray(MakeDATA(train, seq_len)).transpose(0, 2, 1)\n",
    "cond_train_seq = np.asarray(MakeDATA(cond_train, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "val_seq = np.asarray(MakeDATA(val, seq_len)).transpose(0, 2, 1)\n",
    "cond_val_seq = np.asarray(MakeDATA(cond_val, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "test_seq = np.asarray(MakeDATA(test, seq_len)).transpose(0, 2, 1)\n",
    "cond_test_seq = np.asarray(MakeDATA(cond_test, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "train_seq.shape, cond_train_seq.shape, test_seq.shape, cond_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc99567",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(train_seq), torch.from_numpy(cond_train_seq))\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(val_seq), torch.from_numpy(cond_val_seq))\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_seq), torch.from_numpy(cond_test_seq))\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2910dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_val, real_cond_data_val = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6d121",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660f5b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model = TransEncoder(features=train_seq.shape[1], latent_dim=latent_dim, num_heads=n_heads, num_layers=num_layers, cond_features=cond_train_seq.shape[1], cond_model=cond_model, device=device, seq_len=seq_len)\n",
    "\n",
    "ddpm = GaussianDiffusion1D(model, seq_length=seq_len, timesteps=timesteps, objective=objective, loss_type='l2', beta_schedule=beta_schedule)\n",
    "ddpm = ddpm.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(logging_dir, \"tensorboard/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b67e3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2338d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, (data, cond_data) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        cond_data = cond_data.float()\n",
    "        cond_data = cond_data.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = ddpm(data, cond_data)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddca6ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815af49330d140e3b8b2b7d3f0553f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\n",
      "Loss train 0.041459399074316027 val 0.1467375010251999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:05<00:00, 173.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2:\n",
      "Loss train 0.01630663874000311 val 0.13773733377456665\n",
      "epoch 3:\n",
      "Loss train 0.011425980441272259 val 0.14753961563110352\n",
      "epoch 4:\n",
      "Loss train 0.009352367296814918 val 0.14565706253051758\n",
      "epoch 5:\n",
      "Loss train 0.008461471766233445 val 0.154198557138443\n",
      "epoch 6:\n",
      "Loss train 0.007771594103425741 val 0.15368127822875977\n",
      "epoch 7:\n",
      "Loss train 0.007254608061164618 val 0.1591373085975647\n",
      "epoch 8:\n",
      "Loss train 0.006864613592624664 val 0.15810520946979523\n",
      "epoch 9:\n",
      "Loss train 0.006627203352749348 val 0.15803544223308563\n",
      "epoch 10:\n",
      "Loss train 0.006319274805486203 val 0.15592558681964874\n",
      "epoch 11:\n",
      "Loss train 0.006160236585885286 val 0.15190373361110687\n",
      "epoch 12:\n",
      "Loss train 0.005936862651258707 val 0.16060446202754974\n",
      "epoch 13:\n",
      "Loss train 0.005723070301115513 val 0.15532884001731873\n",
      "epoch 14:\n",
      "Loss train 0.00568572198599577 val 0.15767322480678558\n",
      "epoch 15:\n",
      "Loss train 0.00547955672442913 val 0.15521934628486633\n",
      "epoch 16:\n",
      "Loss train 0.0055160635262727735 val 0.16168494522571564\n",
      "epoch 17:\n",
      "Loss train 0.005414167154580355 val 0.16051970422267914\n",
      "epoch 18:\n",
      "Loss train 0.005422653913497924 val 0.15728403627872467\n",
      "epoch 19:\n",
      "Loss train 0.00522317112237215 val 0.14879703521728516\n",
      "epoch 20:\n",
      "Loss train 0.005237462632358074 val 0.15459835529327393\n",
      "epoch 21:\n",
      "Loss train 0.005138737391680479 val 0.1649433821439743\n",
      "epoch 22:\n",
      "Loss train 0.005069963071495294 val 0.155523881316185\n",
      "epoch 23:\n",
      "Loss train 0.00493026753142476 val 0.15867578983306885\n",
      "epoch 24:\n",
      "Loss train 0.004975830029696226 val 0.15267466008663177\n",
      "epoch 25:\n",
      "Loss train 0.004791764907538891 val 0.16015557944774628\n",
      "epoch 26:\n",
      "Loss train 0.004755270630121231 val 0.16490903496742249\n",
      "epoch 27:\n",
      "Loss train 0.004758272184059024 val 0.15093012154102325\n",
      "epoch 28:\n",
      "Loss train 0.004625762052834034 val 0.1620023548603058\n",
      "epoch 29:\n",
      "Loss train 0.0046607360281050204 val 0.15196534991264343\n",
      "epoch 30:\n",
      "Loss train 0.004618064038455486 val 0.15329685807228088\n",
      "epoch 31:\n",
      "Loss train 0.004475699435919523 val 0.16380675137043\n",
      "epoch 32:\n",
      "Loss train 0.00461196718737483 val 0.16516657173633575\n",
      "epoch 33:\n",
      "Loss train 0.004447863085195423 val 0.158528670668602\n",
      "epoch 34:\n",
      "Loss train 0.00444657376781106 val 0.17019042372703552\n",
      "epoch 35:\n",
      "Loss train 0.004390728879719972 val 0.1558062583208084\n",
      "epoch 36:\n",
      "Loss train 0.0043486194796860216 val 0.1559232622385025\n",
      "epoch 37:\n",
      "Loss train 0.0043577621132135395 val 0.15987317264080048\n",
      "epoch 38:\n",
      "Loss train 0.00439163238927722 val 0.15273387730121613\n",
      "epoch 39:\n",
      "Loss train 0.004354714596644044 val 0.15674179792404175\n",
      "epoch 40:\n",
      "Loss train 0.0042164449859410525 val 0.16425560414791107\n",
      "epoch 41:\n",
      "Loss train 0.004270562388002872 val 0.15396004915237427\n",
      "epoch 42:\n",
      "Loss train 0.004197815328836441 val 0.15757744014263153\n",
      "epoch 43:\n",
      "Loss train 0.004149026436731219 val 0.1595265418291092\n",
      "epoch 44:\n",
      "Loss train 0.004174807777628303 val 0.15491095185279846\n",
      "epoch 45:\n",
      "Loss train 0.004165948649868369 val 0.1489565670490265\n",
      "epoch 46:\n",
      "Loss train 0.004116207674145699 val 0.1631784439086914\n",
      "epoch 47:\n",
      "Loss train 0.004219791373237967 val 0.15854081511497498\n",
      "epoch 48:\n",
      "Loss train 0.004057598760351539 val 0.14445340633392334\n",
      "epoch 49:\n",
      "Loss train 0.004056337494403124 val 0.15230689942836761\n",
      "epoch 50:\n",
      "Loss train 0.004007382841780782 val 0.15095360577106476\n",
      "epoch 51:\n",
      "Loss train 0.004035555090755224 val 0.15511706471443176\n",
      "epoch 52:\n",
      "Loss train 0.003990707945078611 val 0.1593286097049713\n",
      "epoch 53:\n",
      "Loss train 0.0040529259424656626 val 0.1562172919511795\n",
      "epoch 54:\n",
      "Loss train 0.00398453770391643 val 0.15691278874874115\n",
      "epoch 55:\n",
      "Loss train 0.004017112812027335 val 0.14960508048534393\n",
      "epoch 56:\n",
      "Loss train 0.004010582260787487 val 0.15515278279781342\n",
      "epoch 57:\n",
      "Loss train 0.003901496892794967 val 0.16326934099197388\n",
      "epoch 58:\n",
      "Loss train 0.003963808964937926 val 0.16117317974567413\n",
      "epoch 59:\n",
      "Loss train 0.00398687325976789 val 0.14933499693870544\n",
      "epoch 60:\n",
      "Loss train 0.003878853652626276 val 0.15916183590888977\n",
      "epoch 61:\n",
      "Loss train 0.003941050311550498 val 0.14933526515960693\n",
      "epoch 62:\n",
      "Loss train 0.003950235078111291 val 0.15219800174236298\n",
      "epoch 63:\n",
      "Loss train 0.0038065719809383153 val 0.15144909918308258\n",
      "epoch 64:\n",
      "Loss train 0.003939737677574158 val 0.16308939456939697\n",
      "epoch 65:\n",
      "Loss train 0.0038991384822875262 val 0.15068699419498444\n",
      "epoch 66:\n",
      "Loss train 0.003844388000667095 val 0.1591460257768631\n",
      "epoch 67:\n",
      "Loss train 0.0038649320490658284 val 0.15228179097175598\n",
      "epoch 68:\n",
      "Loss train 0.0038038458246737717 val 0.15490920841693878\n",
      "epoch 69:\n",
      "Loss train 0.0038308193534612655 val 0.15949907898902893\n",
      "epoch 70:\n",
      "Loss train 0.0038782526943832636 val 0.16152361035346985\n",
      "epoch 71:\n",
      "Loss train 0.0038407022356987 val 0.15924368798732758\n",
      "epoch 72:\n",
      "Loss train 0.003797418937087059 val 0.15793770551681519\n",
      "epoch 73:\n",
      "Loss train 0.0038306551948189735 val 0.15611284971237183\n",
      "epoch 74:\n",
      "Loss train 0.003834709133952856 val 0.16704808175563812\n",
      "epoch 75:\n",
      "Loss train 0.003811719322577119 val 0.1627831608057022\n",
      "epoch 76:\n",
      "Loss train 0.0038238658867776392 val 0.15786153078079224\n",
      "epoch 77:\n",
      "Loss train 0.0037142383698374032 val 0.1639038622379303\n",
      "epoch 78:\n",
      "Loss train 0.0037532306518405674 val 0.1631706804037094\n",
      "epoch 79:\n",
      "Loss train 0.0036973142642527817 val 0.16057616472244263\n",
      "epoch 80:\n",
      "Loss train 0.0038148133400827647 val 0.15471476316452026\n",
      "epoch 81:\n",
      "Loss train 0.0036817454285919668 val 0.15648558735847473\n",
      "epoch 82:\n",
      "Loss train 0.0036633267998695375 val 0.1592646837234497\n",
      "epoch 83:\n",
      "Loss train 0.00375546882674098 val 0.15390212833881378\n",
      "epoch 84:\n",
      "Loss train 0.003697535989806056 val 0.16174624860286713\n",
      "epoch 85:\n",
      "Loss train 0.003665184870362282 val 0.1611310988664627\n",
      "epoch 86:\n",
      "Loss train 0.0036573494207113983 val 0.15851356089115143\n",
      "epoch 87:\n",
      "Loss train 0.0036174890510737895 val 0.1578742265701294\n",
      "epoch 88:\n",
      "Loss train 0.0036326601337641476 val 0.15540073812007904\n",
      "epoch 89:\n",
      "Loss train 0.0036789405047893526 val 0.15794411301612854\n",
      "epoch 90:\n",
      "Loss train 0.0036710868421941994 val 0.1614210456609726\n",
      "epoch 91:\n",
      "Loss train 0.0036865863278508185 val 0.15447412431240082\n",
      "epoch 92:\n",
      "Loss train 0.003517484717071056 val 0.1619836390018463\n",
      "epoch 93:\n",
      "Loss train 0.0036448153778910635 val 0.16842231154441833\n",
      "epoch 94:\n",
      "Loss train 0.0036580433920025824 val 0.166852205991745\n",
      "epoch 95:\n",
      "Loss train 0.0036084041744470596 val 0.1581100970506668\n",
      "epoch 96:\n",
      "Loss train 0.003525274857878685 val 0.16488656401634216\n",
      "epoch 97:\n",
      "Loss train 0.0035254134442657234 val 0.1599040925502777\n",
      "epoch 98:\n",
      "Loss train 0.0036576680168509484 val 0.16412784159183502\n",
      "epoch 99:\n",
      "Loss train 0.00357792828977108 val 0.16671669483184814\n",
      "epoch 100:\n",
      "Loss train 0.0036252155415713787 val 0.17016811668872833\n",
      "epoch 101:\n",
      "Loss train 0.0034668126814067363 val 0.16083581745624542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:05<00:00, 177.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102:\n",
      "Loss train 0.0034939838387072087 val 0.16038890182971954\n",
      "epoch 103:\n",
      "Loss train 0.003495155746117234 val 0.17183315753936768\n",
      "epoch 104:\n",
      "Loss train 0.003629349747672677 val 0.16145512461662292\n",
      "epoch 105:\n",
      "Loss train 0.0034245638828724625 val 0.16321660578250885\n",
      "epoch 106:\n",
      "Loss train 0.0035039805937558412 val 0.17108821868896484\n",
      "epoch 107:\n",
      "Loss train 0.0035908659137785437 val 0.15962180495262146\n",
      "epoch 108:\n",
      "Loss train 0.0036281929202377798 val 0.1574808955192566\n",
      "epoch 109:\n",
      "Loss train 0.003570501361042261 val 0.1605554223060608\n",
      "epoch 110:\n",
      "Loss train 0.003487955246120691 val 0.16298268735408783\n",
      "epoch 111:\n",
      "Loss train 0.0035140632018446924 val 0.15753456950187683\n",
      "epoch 112:\n",
      "Loss train 0.0035043385419994593 val 0.16908805072307587\n",
      "epoch 113:\n",
      "Loss train 0.003516729462891817 val 0.16653528809547424\n",
      "epoch 114:\n",
      "Loss train 0.0034769150242209435 val 0.166618213057518\n",
      "epoch 115:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m      9\u001b[0m cond_data \u001b[38;5;241m=\u001b[39m cond_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\OU\\master\\afstuderen\\Thesis\\src\\ddpm\\ddpm.py:382\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.forward\u001b[1;34m(self, img, cond_data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m img \u001b[38;5;241m=\u001b[39m normalize_to_neg_one_to_one(img)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# print(f'shape after normalizing: {img.shape}')\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_losses(img, cond_data, t, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\OU\\master\\afstuderen\\Thesis\\src\\ddpm\\ddpm.py:355\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.p_losses\u001b[1;34m(self, x_start, cond_data, t, noise)\u001b[0m\n\u001b[0;32m    353\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, sqrt_cumprod_alpha, cond_data)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_noise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    358\u001b[0m     target \u001b[38;5;241m=\u001b[39m noise\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\OU\\master\\afstuderen\\Thesis\\src\\models\\transformer_encoder_x_attn.py:131\u001b[0m, in \u001b[0;36mTransEncoder.forward\u001b[1;34m(self, x, t, cond_input)\u001b[0m\n\u001b[0;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [seq_len, batch, features]\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Time embedding\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb_timestep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m time_added_data \u001b[38;5;241m=\u001b[39m embed \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m    133\u001b[0m time_added_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(time_added_data)\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\OU\\master\\afstuderen\\Thesis\\src\\embeddings\\timestep.py:17\u001b[0m, in \u001b[0;36mTimestepEmbedder.forward\u001b[1;34m(self, timesteps)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, timesteps):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_pos_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "best_val_loss = 1_000_000.\n",
    "save_dir =  os.path.join(logging_dir, \"weights/\")\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    print('epoch {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (val_data, val_cond) in enumerate(val_loader):\n",
    "            val_data = val_data.to(device)\n",
    "        \n",
    "            val_cond = val_cond.float()\n",
    "            val_cond = val_cond.to(device)\n",
    "            val_loss = ddpm(val_data, val_cond)\n",
    "            \n",
    "            running_val_loss += val_loss\n",
    "    \n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    print('Loss train {} val {}'.format(avg_loss, avg_val_loss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_val_loss },\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    writer.flush()\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch_number\n",
    "        best_ddpm_model = ddpm.state_dict()\n",
    "        best_optim_state_dict = optim.state_dict()\n",
    "        \n",
    "    \n",
    "    if epoch_number % save_rate == 0:\n",
    "        generated_sample = ddpm.sample(batch_size, real_cond_data_val.to(device))\n",
    "        generated_sample = generated_sample.cpu().numpy()\n",
    "        \n",
    "        plot_kde_samples(generated_sample, real_data_val,show=False, fpath=os.path.join(logging_dir, \"kde/\", f\"kde_epoch_{epoch_number}.png\"), epoch=epoch_number)\n",
    "        \n",
    "        model_name = f\"model_epoch_{epoch_number}_val_{avg_val_loss:.4f}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch_number,\n",
    "            'diffusion_state_dict': ddpm.state_dict(),\n",
    "            'diffusion_optim_state_dict': optim.state_dict()},\n",
    "            os.path.join(save_dir, model_name))\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"best_model.pth\"\n",
    "torch.save({\n",
    "    'epoch': best_epoch,\n",
    "    'diffusion_state_dict': best_ddpm_model,\n",
    "    'diffusion_optim_state_dict': best_optim_state_dict},\n",
    "    os.path.join(logging_dir, \"weights/\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36754b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm.load_state_dict(torch.load(os.path.join(logging_dir, \"weights/\", \"best_model.pth\"))[\"diffusion_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for kde_plot in os.listdir(os.path.join(logging_dir, \"kde/\")):\n",
    "    paths.append(os.path.join(logging_dir, \"kde/\", kde_plot))\n",
    "\n",
    "make_gif_from_images(paths, os.path.join(logging_dir, \"kde/\", \"kde_progression.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a252d4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98900634",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_test, real_cond_data_test = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = ddpm.sample(batch_size, real_cond_data_test.to(device))\n",
    "    samples = samples.cpu().numpy()\n",
    "\n",
    "print(f\"Samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d638ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizual_comparison(samples, real_data_test, os.path.join(logging_dir, \"viz/\", \"pca_umap_tsne_all_batches.png\"), use_all_data=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizual_comparison(samples, real_data_test, os.path.join(logging_dir, \"viz/\", \"pca_umap_tsne_per_batch.png\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_histogram_per_customer(samples, real_data_test, fpath=os.path.join(logging_dir, \"mmd/\", \"mmd.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jsd_per_customer(samples, real_data_test, os.path.join(logging_dir, \"jsd/\", \"jsd.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde_samples(samples, real_data_test, fpath=os.path.join(logging_dir, \"kde/\", f\"kde.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = np.random.randint(0, 256) \n",
    "customer_indices = np.random.randint(0, 15, size=15)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample = ddpm.sample(1, real_cond_data_test.to(device)) \n",
    "    sample = sample.cpu().numpy() \n",
    "\n",
    "for i in customer_indices:\n",
    "    axs[0].plot(sample[0, i], alpha=0.7, label=f'Customer {i}')\n",
    "    axs[1].plot(real_data_test[0, i], alpha=0.7, label=f'Customer {i}')\n",
    "\n",
    "axs[0].set_title(\"Generated Data\")\n",
    "axs[1].set_title(\"Real Data\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(logging_dir, \"ts_sample/\", \"samples.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
