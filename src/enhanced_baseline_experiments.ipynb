{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482433e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from models.enhanced_baseline import EnhancedBaseLineModel\n",
    "from ddpm.ddpm import GaussianDiffusion1D\n",
    "from evaluation.evaluation import vizual_comparison, plot_jsd_per_customer, plot_kde_samples, make_gif_from_images, mmd_histogram_per_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe5852",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "seq_len = 12\n",
    "batch_size = 256\n",
    "k = 15\n",
    "\n",
    "#NN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "latent_dim = 1000\n",
    "cond_model = \"mlp\"\n",
    "num_layers = 6\n",
    "n_heads = 8\n",
    "lr = 5*16**-4\n",
    "decay_rate = 0.9\n",
    "epochs = 2000\n",
    "save_rate = 100\n",
    "\n",
    "## DDPM\n",
    "timesteps = 1000\n",
    "beta_schedule = \"linear\"\n",
    "objective = \"pred_noise\"\n",
    "\n",
    "## Logging\n",
    "experiment_name = \"enhanced_baseline_cosine\"\n",
    "logging_dir = f\"./logging/{experiment_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745901d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "    os.makedirs(os.path.join(logging_dir, \"viz/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"jsd/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"kde/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"tensorboard/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"weights/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"ts_sample/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"mmd/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba0f3e",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1684f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = \"./preprocessing/data/customer_led_network_revolution/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7dc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDATA(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        seq_data = []\n",
    "        for i in range(len(data) - seq_len + 1):\n",
    "            x = data[i : i + seq_len]\n",
    "            seq_data.append(x)\n",
    "        self.samples = np.asarray(seq_data, dtype=np.float32) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75226034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    cluster_labels = kmeans.fit_predict(data.T)\n",
    "    \n",
    "    clustered_data = []\n",
    "    for cluster in range(kmeans.n_clusters):\n",
    "        cluster_data = data.iloc[:, cluster_labels == cluster].mean(axis=1)\n",
    "        clustered_data.append(cluster_data)\n",
    "        \n",
    "    return pd.DataFrame(clustered_data).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f13fa2",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70af45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"test.csv\"))\n",
    "cond_train = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_train.csv\"))\n",
    "cond_val = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_val.csv\"))\n",
    "cond_test = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(\"Date and Time of capture\", axis=1, inplace=True)\n",
    "val.drop(\"Date and Time of capture\", axis=1, inplace=True)\n",
    "test.drop(\"Date and Time of capture\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a431307",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cluster(train, k)\n",
    "test = cluster(test, k)\n",
    "val = cluster(val, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239773df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, val.shape, test.shape, cond_train.shape, cond_val.shape, cond_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd074c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = np.asarray(MakeDATA(train, seq_len)).transpose(0, 2, 1)\n",
    "cond_train_seq = np.asarray(MakeDATA(cond_train, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "val_seq = np.asarray(MakeDATA(val, seq_len)).transpose(0, 2, 1)\n",
    "cond_val_seq = np.asarray(MakeDATA(cond_val, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "test_seq = np.asarray(MakeDATA(test, seq_len)).transpose(0, 2, 1)\n",
    "cond_test_seq = np.asarray(MakeDATA(cond_test, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "train_seq.shape, cond_train_seq.shape, test_seq.shape, cond_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff26c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(train_seq), torch.from_numpy(cond_train_seq))\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(val_seq), torch.from_numpy(cond_val_seq))\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_seq), torch.from_numpy(cond_test_seq))\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22122eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_val, real_cond_data_val = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2956b",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedBaseLineModel(seq_len=seq_len, hidden_dim=latent_dim, cond_dim=cond_train_seq.shape[1], cond_model=cond_model, device=device, channels=train_seq.shape[1])\n",
    "\n",
    "ddpm = GaussianDiffusion1D(model, seq_length = seq_len, timesteps = timesteps, objective = objective, loss_type = 'l2', beta_schedule = beta_schedule)\n",
    "ddpm = ddpm.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr = lr)\n",
    "scheduler = lr_scheduler.StepLR(optim, step_size=1000, gamma=0.9)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(logging_dir, \"tensorboard/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b485dc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://medium.com/@heyamit10/exponential-moving-average-ema-in-pytorch-eb8b6f1718eb\n",
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        \"\"\"\n",
    "        Initialize EMA class to manage exponential moving average of model parameters.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model for which EMA will track parameters.\n",
    "            decay (float): Decay rate, typically a value close to 1, e.g., 0.999.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "        # Store initial parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update shadow parameters with exponential decay.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        \"\"\"\n",
    "        Apply shadow (EMA) parameters to model.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore original model parameters from backup.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_dict, ema_shadow, optimizer_dict, filepath):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model_dict,\n",
    "        'ema_state_dict': ema_shadow,\n",
    "        'optimizer_state_dict': optimizer_dict,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, ema, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if 'ema_state_dict' in checkpoint:\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in checkpoint['ema_state_dict']:\n",
    "                ema.shadow[name] = checkpoint['ema_state_dict'][name].clone()\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05520a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, (data, cond_data) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        cond_data = cond_data.float()\n",
    "        cond_data = cond_data.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = ddpm(data, cond_data)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        ema.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = EMA(model, decay=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "best_val_loss = 1_000_000.\n",
    "save_dir =  os.path.join(logging_dir, \"weights/\")\n",
    "\n",
    "ema = EMA(model, decay=decay_rate)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    print('epoch {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    ema.apply_shadow()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (val_data, val_cond) in enumerate(val_loader):\n",
    "            val_data = val_data.to(device)\n",
    "        \n",
    "            val_cond = val_cond.float()\n",
    "            val_cond = val_cond.to(device)\n",
    "            val_loss = ddpm(val_data, val_cond)\n",
    "            \n",
    "            running_val_loss += val_loss\n",
    "    \n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    print('Loss train {} val {}'.format(avg_loss, avg_val_loss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_val_loss },\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    writer.flush()\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_dict = model.state_dict()\n",
    "        best_model_opt = optim.state_dict()\n",
    "        best_model_shadow = ema.shadow\n",
    "        \n",
    "    \n",
    "    if epoch_number % save_rate == 0:\n",
    "        generated_sample = ddpm.sample(batch_size, real_cond_data_val.to(device))\n",
    "        generated_sample = generated_sample.cpu().numpy()\n",
    "        \n",
    "        plot_kde_samples(generated_sample, real_data_val,show=False, fpath=os.path.join(logging_dir, \"kde/\", f\"kde_epoch_{epoch_number}.png\"), epoch=epoch_number)\n",
    "        \n",
    "        model_name = f\"model_epoch_{epoch_number}_val_{avg_val_loss:.4f}.pth\"\n",
    "        save_model(model.state_dict(), ema.shadow, optim.state_dict(), os.path.join(save_dir, model_name))\n",
    "    \n",
    "    ema.restore()\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_model_dict, best_model_shadow, best_model_opt, os.path.join(logging_dir, \"weights/\", \"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbecf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, ema, optim, os.path.join(logging_dir, \"weights/\", \"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f37315",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for kde_plot in os.listdir(os.path.join(logging_dir, \"kde/\")):\n",
    "    paths.append(os.path.join(logging_dir, \"kde/\", kde_plot))\n",
    "\n",
    "make_gif_from_images(paths, os.path.join(logging_dir, \"kde/\", \"kde_progression.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16046b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffa8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_test, real_cond_data_test = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee051c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = ddpm.sample(batch_size, real_cond_data_test.to(device))\n",
    "    samples = samples.cpu().numpy()\n",
    "\n",
    "print(f\"Samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizual_comparison(samples, real_data_test, os.path.join(logging_dir, \"viz/\", \"pca_umap_tsne_all_batches.png\"), use_all_data=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizual_comparison(samples, real_data_test, os.path.join(logging_dir, \"viz/\", \"pca_umap_tsne_per_batch.png\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_histogram_per_customer(samples, real_data_test, fpath=os.path.join(logging_dir, \"mmd/\", \"mmd.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jsd_per_customer(samples, real_data_test, os.path.join(logging_dir, \"jsd/\", \"jsd.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a40031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde_samples(samples, real_data_test, fpath=os.path.join(logging_dir, \"kde/\", f\"kde.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f370bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = np.random.randint(0, 256) \n",
    "customer_indices = np.random.randint(0, 15, size=15)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample = ddpm.sample(1, real_cond_data_test.to(device)) \n",
    "    sample = sample.cpu().numpy() \n",
    "\n",
    "for i in customer_indices:\n",
    "    axs[0].plot(sample[0, i], alpha=0.7, label=f'Customer {i}')\n",
    "    axs[1].plot(real_data_test[0, i], alpha=0.7, label=f'Customer {i}')\n",
    "\n",
    "axs[0].set_title(\"Generated Data\")\n",
    "axs[1].set_title(\"Real Data\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(logging_dir, \"ts_sample/\", \"samples.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a817ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
