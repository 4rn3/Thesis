{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482433e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from models.enhanced_baseline import EnhancedBaseLineModel\n",
    "from ddpm.ddpm import GaussianDiffusion1D\n",
    "from evaluation.evaluation import vizual_comparison, plot_jsd_per_customer, plot_kde_samples, make_gif_from_images, mmd_histogram_per_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe5852",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b94e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "seq_len = 48\n",
    "batch_size = 128\n",
    "k = 15\n",
    "\n",
    "#NN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "latent_dim = 1000\n",
    "cond_model = \"mlp\"\n",
    "num_layers = 6\n",
    "n_heads = 8\n",
    "lr = 5e-5\n",
    "decay_rate = 0.9\n",
    "epochs = 2000\n",
    "save_rate = 100\n",
    "\n",
    "## DDPM\n",
    "timesteps = 1000\n",
    "beta_schedule = \"cosine\"\n",
    "objective = \"pred_noise\"\n",
    "\n",
    "## Logging\n",
    "experiment_name = \"enhanced_dev\"\n",
    "logging_dir = f\"./logging/{experiment_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745901d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "    os.makedirs(os.path.join(logging_dir, \"viz/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"jsd/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"kde/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"tensorboard/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"weights/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"ts_sample/\"))\n",
    "    os.makedirs(os.path.join(logging_dir, \"mmd/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba0f3e",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1684f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = \"./preprocessing/data/customer_led_network_revolution/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7dc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDATA(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        seq_data = []\n",
    "        for i in range(len(data) - seq_len + 1):\n",
    "            x = data[i : i + seq_len]\n",
    "            seq_data.append(x)\n",
    "        self.samples = np.asarray(seq_data, dtype=np.float32) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75226034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    cluster_labels = kmeans.fit_predict(data.T)\n",
    "    \n",
    "    clustered_data = []\n",
    "    for cluster in range(kmeans.n_clusters):\n",
    "        cluster_data = data.iloc[:, cluster_labels == cluster].mean(axis=1)\n",
    "        clustered_data.append(cluster_data)\n",
    "        \n",
    "    return pd.DataFrame(clustered_data).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f13fa2",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70af45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"train.csv\"))\n",
    "val = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"val.csv\"))\n",
    "test = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"test.csv\"))\n",
    "cond_train = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_train.csv\"))\n",
    "cond_val = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_val.csv\"))\n",
    "cond_test = pd.read_csv(os.path.join(PREPROCESSED_DIR, \"cond_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a963e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(\"Date and Time of capture\", axis=1, inplace=True)\n",
    "val.drop(\"Date and Time of capture\", axis=1, inplace=True)\n",
    "test.drop(\"Date and Time of capture\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a431307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "train = cluster(train, k)\n",
    "test = cluster(test, k)\n",
    "val = cluster(val, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "239773df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33984, 15), (6796, 15), (1700, 15), (33984, 27), (6796, 27), (1700, 27))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape, cond_train.shape, cond_val.shape, cond_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd074c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33937, 15, 48), (33937, 27, 48), (1653, 15, 48), (1653, 27, 48))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq = np.asarray(MakeDATA(train, seq_len)).transpose(0, 2, 1)\n",
    "cond_train_seq = np.asarray(MakeDATA(cond_train, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "val_seq = np.asarray(MakeDATA(val, seq_len)).transpose(0, 2, 1)\n",
    "cond_val_seq = np.asarray(MakeDATA(cond_val, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "test_seq = np.asarray(MakeDATA(test, seq_len)).transpose(0, 2, 1)\n",
    "cond_test_seq = np.asarray(MakeDATA(cond_test, seq_len)).transpose(0, 2, 1)\n",
    "\n",
    "train_seq.shape, cond_train_seq.shape, test_seq.shape, cond_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff26c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(train_seq), torch.from_numpy(cond_train_seq))\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(val_seq), torch.from_numpy(cond_val_seq))\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_seq), torch.from_numpy(cond_test_seq))\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22122eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_val, real_cond_data_val = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2956b",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844817f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             torch.nn.init.zeros_(m.bias)\n",
    "#     elif isinstance(m, nn.LSTM):\n",
    "#         for name, param in m.named_parameters():\n",
    "#             if 'weight' in name:\n",
    "#                 torch.nn.init.xavier_uniform_(param.data)\n",
    "#             elif 'bias' in name:\n",
    "#                 torch.nn.init.zeros_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6576c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedBaseLineModel(features=train_seq.shape[1], hidden_dim=latent_dim, cond_dim=cond_train_seq.shape[1], cond_model=cond_model, device=device, channels=train_seq.shape[1])\n",
    "\n",
    "# model.apply(init_weights)\n",
    "\n",
    "ddpm = GaussianDiffusion1D(model, seq_length = seq_len, timesteps = timesteps, objective = objective, loss_type = 'l2', beta_schedule = beta_schedule)\n",
    "ddpm = ddpm.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr = lr)\n",
    "scheduler = lr_scheduler.StepLR(optim, step_size=1000, gamma=0.9)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(logging_dir, \"tensorboard/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b485dc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4fb58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://medium.com/@heyamit10/exponential-moving-average-ema-in-pytorch-eb8b6f1718eb\n",
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        \"\"\"\n",
    "        Initialize EMA class to manage exponential moving average of model parameters.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model for which EMA will track parameters.\n",
    "            decay (float): Decay rate, typically a value close to 1, e.g., 0.999.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "        # Store initial parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update shadow parameters with exponential decay.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        \"\"\"\n",
    "        Apply shadow (EMA) parameters to model.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore original model parameters from backup.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dd4bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_dict, ema_shadow, optimizer_dict, filepath):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model_dict,\n",
    "        'ema_state_dict': ema_shadow,\n",
    "        'optimizer_state_dict': optimizer_dict,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "966a5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, ema, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if 'ema_state_dict' in checkpoint:\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in checkpoint['ema_state_dict']:\n",
    "                ema.shadow[name] = checkpoint['ema_state_dict'][name].clone()\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05520a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, (data, cond_data) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        cond_data = cond_data.float()\n",
    "        cond_data = cond_data.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = ddpm(data, cond_data)\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        # ################# DEBUG ########################\n",
    "        # \"\"\"\n",
    "        # Exploding gradients: Norms become very large (e.g., >100 or even inf).\n",
    "        # Vanishing gradients: Norms shrink toward zero.\n",
    "        # \"\"\"\n",
    "        # total_norm = 0.0\n",
    "        # for p in model.parameters():\n",
    "        #     if p.grad is not None:\n",
    "        #         param_norm = p.grad.data.norm(2)\n",
    "        #         total_norm += param_norm.item() ** 2\n",
    "        # total_norm = total_norm ** 0.5\n",
    "        # tb_writer.add_scalar('Gradient Norm', total_norm, epoch_index * len(train_loader) + i)\n",
    "        \n",
    "        # if total_norm > 1e3 or torch.isnan(torch.tensor(total_norm)):\n",
    "        #     print(f\"High or NaN gradient norm at step {i}: {total_norm:.4f}\")\n",
    "            \n",
    "        # ################# DEBUG ########################\n",
    "        \n",
    "        optim.step()\n",
    "        ema.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d058f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = EMA(model, decay=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1123ab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677e3ddf061d43c9819d1c19269d67d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:1410.)\n",
      "  result = _VF.lstm(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train 1.3463373770713807 val 1.2723420858383179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2:\n",
      "Loss train 0.11254887318611145 val 1.298607349395752\n",
      "epoch 3:\n",
      "Loss train 0.08954696607589721 val 1.286897897720337\n",
      "epoch 4:\n",
      "Loss train 0.07518663370609284 val 1.429446816444397\n",
      "epoch 5:\n",
      "Loss train 0.05480870220065117 val 1.1906548738479614\n",
      "epoch 6:\n",
      "Loss train 0.0468406862616539 val 1.316590428352356\n",
      "epoch 7:\n",
      "Loss train 0.036303610414266586 val 1.2452750205993652\n",
      "epoch 8:\n",
      "Loss train 0.03073615598678589 val 1.0947567224502563\n",
      "epoch 9:\n",
      "Loss train 0.028694360673427583 val 1.4266196489334106\n",
      "epoch 10:\n",
      "Loss train 0.02436915910243988 val 0.8712239861488342\n",
      "epoch 11:\n",
      "Loss train 0.019972516730427742 val 0.8117837309837341\n",
      "epoch 12:\n",
      "Loss train 0.02157553370296955 val 2.108886241912842\n",
      "epoch 13:\n",
      "Loss train 0.017647766202688216 val 0.8306003212928772\n",
      "epoch 14:\n",
      "Loss train 0.01647813509404659 val 0.6700195074081421\n",
      "epoch 15:\n",
      "Loss train 0.015940967231988908 val 1.0412626266479492\n",
      "epoch 16:\n",
      "Loss train 0.01641804249584675 val 1.2835571765899658\n",
      "epoch 17:\n",
      "Loss train 0.014072835259139539 val 1.050349235534668\n",
      "epoch 18:\n",
      "Loss train 0.014871406264603138 val 0.9608449339866638\n",
      "epoch 19:\n",
      "Loss train 0.013139159515500069 val 0.7868735790252686\n",
      "epoch 20:\n",
      "Loss train 0.012388738483190536 val 0.7843127250671387\n",
      "epoch 21:\n",
      "Loss train 0.012630529806017875 val 1.2710918188095093\n",
      "epoch 22:\n",
      "Loss train 0.01113799210637808 val 0.768508791923523\n",
      "epoch 23:\n",
      "Loss train 0.010147924982011319 val 0.3275148570537567\n",
      "epoch 24:\n",
      "Loss train 0.009115923214703798 val 0.3484656512737274\n",
      "epoch 25:\n",
      "Loss train 0.008106908138841391 val 0.4651015102863312\n",
      "epoch 26:\n",
      "Loss train 0.00909690423309803 val 0.48840880393981934\n",
      "epoch 27:\n",
      "Loss train 0.007400846317410469 val 0.38649076223373413\n",
      "epoch 28:\n",
      "Loss train 0.007267453495413065 val 0.3614853024482727\n",
      "epoch 29:\n",
      "Loss train 0.007147596970200539 val 0.2981814742088318\n",
      "epoch 30:\n",
      "Loss train 0.0066183007061481475 val 0.4324341118335724\n",
      "epoch 31:\n",
      "Loss train 0.00637654560059309 val 0.30651211738586426\n",
      "epoch 32:\n",
      "Loss train 0.006545183476060629 val 0.42089760303497314\n",
      "epoch 33:\n",
      "Loss train 0.005997360646724701 val 0.28904759883880615\n",
      "epoch 34:\n",
      "Loss train 0.00604854417592287 val 0.33374595642089844\n",
      "epoch 35:\n",
      "Loss train 0.005865720771253109 val 0.2011454999446869\n",
      "epoch 36:\n",
      "Loss train 0.0058104900829494 val 0.17408137023448944\n",
      "epoch 37:\n",
      "Loss train 0.0055124167464673515 val 0.299221396446228\n",
      "epoch 38:\n",
      "Loss train 0.005420214399695396 val 0.244441956281662\n",
      "epoch 39:\n",
      "Loss train 0.00538526862859726 val 0.23617878556251526\n",
      "epoch 40:\n",
      "Loss train 0.005093996498733759 val 0.15710030496120453\n",
      "epoch 41:\n",
      "Loss train 0.005051390282809734 val 0.16702543199062347\n",
      "epoch 42:\n",
      "Loss train 0.004973590485751629 val 0.1671760380268097\n",
      "epoch 43:\n",
      "Loss train 0.004851081177592278 val 0.16088373959064484\n",
      "epoch 44:\n",
      "Loss train 0.004926372975111008 val 0.15918579697608948\n",
      "epoch 45:\n",
      "Loss train 0.004681324815377593 val 0.16582228243350983\n",
      "epoch 46:\n",
      "Loss train 0.004614410838112235 val 0.1775161623954773\n",
      "epoch 47:\n",
      "Loss train 0.004729048708453774 val 0.14299307763576508\n",
      "epoch 48:\n",
      "Loss train 0.004667407112196088 val 0.15213826298713684\n",
      "epoch 49:\n",
      "Loss train 0.004649413315579295 val 0.1446906477212906\n",
      "epoch 50:\n",
      "Loss train 0.004665240548551082 val 0.14361901581287384\n",
      "epoch 51:\n",
      "Loss train 0.004345841497182846 val 0.13002251088619232\n",
      "epoch 52:\n",
      "Loss train 0.004193535136058926 val 0.13300003111362457\n",
      "epoch 53:\n",
      "Loss train 0.004071851780638099 val 0.13533712923526764\n",
      "epoch 54:\n",
      "Loss train 0.004450892020016909 val 0.12905597686767578\n",
      "epoch 55:\n",
      "Loss train 0.00412855432741344 val 0.13783611357212067\n",
      "epoch 56:\n",
      "Loss train 0.004189898191019893 val 0.12951092422008514\n",
      "epoch 57:\n",
      "Loss train 0.004091011885553599 val 0.1379530429840088\n",
      "epoch 58:\n",
      "Loss train 0.004201786739751696 val 0.1476026177406311\n",
      "epoch 59:\n",
      "Loss train 0.0041925519984215495 val 0.1402340978384018\n",
      "epoch 60:\n",
      "Loss train 0.004126005824655294 val 0.1396525353193283\n",
      "epoch 61:\n",
      "Loss train 0.004395886128768325 val 0.13016369938850403\n",
      "epoch 62:\n",
      "Loss train 0.003907681353390217 val 0.12550337612628937\n",
      "epoch 63:\n",
      "Loss train 0.004180080292746424 val 0.12938192486763\n",
      "epoch 64:\n",
      "Loss train 0.004053614113479853 val 0.1292562037706375\n",
      "epoch 65:\n",
      "Loss train 0.00398209336027503 val 0.12537920475006104\n",
      "epoch 66:\n",
      "Loss train 0.003972751595079899 val 0.1317213475704193\n",
      "epoch 67:\n",
      "Loss train 0.003992737527936697 val 0.13229231536388397\n",
      "epoch 68:\n",
      "Loss train 0.004029219346120954 val 0.13200856745243073\n",
      "epoch 69:\n",
      "Loss train 0.0038120386078953745 val 0.13760994374752045\n",
      "epoch 70:\n",
      "Loss train 0.004042742568999529 val 0.12470906227827072\n",
      "epoch 71:\n",
      "Loss train 0.004113075848668814 val 0.130019873380661\n",
      "epoch 72:\n",
      "Loss train 0.003783162919804454 val 0.1310374140739441\n",
      "epoch 73:\n",
      "Loss train 0.0036938658729195593 val 0.12827827036380768\n",
      "epoch 74:\n",
      "Loss train 0.003758556457236409 val 0.1281023919582367\n",
      "epoch 75:\n",
      "Loss train 0.0036682974211871625 val 0.11792793869972229\n",
      "epoch 76:\n",
      "Loss train 0.0037080874387174843 val 0.13023848831653595\n",
      "epoch 77:\n",
      "Loss train 0.0036431544795632364 val 0.12533855438232422\n",
      "epoch 78:\n",
      "Loss train 0.003653285199776292 val 0.1245785802602768\n",
      "epoch 79:\n",
      "Loss train 0.0035768998321145775 val 0.1271117776632309\n",
      "epoch 80:\n",
      "Loss train 0.0037957393787801264 val 0.13194268941879272\n",
      "epoch 81:\n",
      "Loss train 0.0036383355520665647 val 0.12310586869716644\n",
      "epoch 82:\n",
      "Loss train 0.0035469556599855424 val 0.12069449573755264\n",
      "epoch 83:\n",
      "Loss train 0.003509294368326664 val 0.1259046047925949\n",
      "epoch 84:\n",
      "Loss train 0.0035952234752476215 val 0.12818969786167145\n",
      "epoch 85:\n",
      "Loss train 0.00368441872484982 val 0.12067108601331711\n",
      "epoch 86:\n",
      "Loss train 0.0035750132519751787 val 0.12249043583869934\n",
      "epoch 87:\n",
      "Loss train 0.003407571593299508 val 0.1251259297132492\n",
      "epoch 88:\n",
      "Loss train 0.003599122231826186 val 0.11791198700666428\n",
      "epoch 89:\n",
      "Loss train 0.0034743620548397303 val 0.12884587049484253\n",
      "epoch 90:\n",
      "Loss train 0.0035011301543563605 val 0.12853002548217773\n",
      "epoch 91:\n",
      "Loss train 0.00354685134626925 val 0.1232772171497345\n",
      "epoch 92:\n",
      "Loss train 0.0033990398086607455 val 0.1304338574409485\n",
      "epoch 93:\n",
      "Loss train 0.003479253616183996 val 0.11778807640075684\n",
      "epoch 94:\n",
      "Loss train 0.0033944580275565386 val 0.12011756747961044\n",
      "epoch 95:\n",
      "Loss train 0.003499440062791109 val 0.1243344098329544\n",
      "epoch 96:\n",
      "Loss train 0.003320504553616047 val 0.12311866134405136\n",
      "epoch 97:\n",
      "Loss train 0.0033907656110823154 val 0.1284199357032776\n",
      "epoch 98:\n",
      "Loss train 0.003444074662402272 val 0.12855516374111176\n",
      "epoch 99:\n",
      "Loss train 0.0034927147459238765 val 0.12174605578184128\n",
      "epoch 100:\n",
      "Loss train 0.003262519916519523 val 0.12609228491783142\n",
      "epoch 101:\n",
      "Loss train 0.003298264492303133 val 0.1222086250782013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:43<00:00, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102:\n",
      "Loss train 0.003325168455019593 val 0.13092674314975739\n",
      "epoch 103:\n",
      "Loss train 0.0033609915636479853 val 0.13483203947544098\n",
      "epoch 104:\n",
      "Loss train 0.0032419493962079285 val 0.12412938475608826\n",
      "epoch 105:\n",
      "Loss train 0.003339411111548543 val 0.12416158616542816\n",
      "epoch 106:\n",
      "Loss train 0.003174264112487435 val 0.13317029178142548\n",
      "epoch 107:\n",
      "Loss train 0.0033208289500325917 val 0.12906573712825775\n",
      "epoch 108:\n",
      "Loss train 0.0034286395329982044 val 0.12547515332698822\n",
      "epoch 109:\n",
      "Loss train 0.003240148330107331 val 0.13292475044727325\n",
      "epoch 110:\n",
      "Loss train 0.0032334995539858935 val 0.12781357765197754\n",
      "epoch 111:\n",
      "Loss train 0.003269212333485484 val 0.13310512900352478\n",
      "epoch 112:\n",
      "Loss train 0.003302158256992698 val 0.13048326969146729\n",
      "epoch 113:\n",
      "Loss train 0.003169633448123932 val 0.13099713623523712\n",
      "epoch 114:\n",
      "Loss train 0.003125088242813945 val 0.1254284828901291\n",
      "epoch 115:\n",
      "Loss train 0.0032639253493398426 val 0.13477428257465363\n",
      "epoch 116:\n",
      "Loss train 0.003196832099929452 val 0.13134926557540894\n",
      "epoch 117:\n",
      "Loss train 0.0032538946885615587 val 0.13921870291233063\n",
      "epoch 118:\n",
      "Loss train 0.003236344665288925 val 0.12911555171012878\n",
      "epoch 119:\n",
      "Loss train 0.003305347705259919 val 0.13175512850284576\n",
      "epoch 120:\n",
      "Loss train 0.0032362059820443393 val 0.13652877509593964\n",
      "epoch 121:\n",
      "Loss train 0.003014463037252426 val 0.12605641782283783\n",
      "epoch 122:\n",
      "Loss train 0.0031675024926662446 val 0.1258692741394043\n",
      "epoch 123:\n",
      "Loss train 0.003163767594844103 val 0.123812235891819\n",
      "epoch 124:\n",
      "Loss train 0.0031334484815597535 val 0.13058534264564514\n",
      "epoch 125:\n",
      "Loss train 0.003131490660831332 val 0.1315867155790329\n",
      "epoch 126:\n",
      "Loss train 0.003169782059267163 val 0.11960422992706299\n",
      "epoch 127:\n",
      "Loss train 0.0030799712203443052 val 0.12981072068214417\n",
      "epoch 128:\n",
      "Loss train 0.0030036289766430854 val 0.12336893379688263\n",
      "epoch 129:\n",
      "Loss train 0.0033028285019099713 val 0.12458895146846771\n",
      "epoch 130:\n",
      "Loss train 0.003195014864206314 val 0.13097505271434784\n",
      "epoch 131:\n",
      "Loss train 0.0030474693020805716 val 0.1307477056980133\n",
      "epoch 132:\n",
      "Loss train 0.0030349177848547695 val 0.12528637051582336\n",
      "epoch 133:\n",
      "Loss train 0.0032509281281381847 val 0.1294400840997696\n",
      "epoch 134:\n",
      "Loss train 0.003104470217600465 val 0.12558481097221375\n",
      "epoch 135:\n",
      "Loss train 0.0031447854842990637 val 0.12210681289434433\n",
      "epoch 136:\n",
      "Loss train 0.003240470027551055 val 0.13250970840454102\n",
      "epoch 137:\n",
      "Loss train 0.003152097774669528 val 0.12470684945583344\n",
      "epoch 138:\n",
      "Loss train 0.003046249309554696 val 0.12735815346240997\n",
      "epoch 139:\n",
      "Loss train 0.0031799033004790544 val 0.12626346945762634\n",
      "epoch 140:\n",
      "Loss train 0.003054227329790592 val 0.1259068101644516\n",
      "epoch 141:\n",
      "Loss train 0.003219660297036171 val 0.13127939403057098\n",
      "epoch 142:\n",
      "Loss train 0.0031618089005351064 val 0.13271372020244598\n",
      "epoch 143:\n",
      "Loss train 0.0030699072983115913 val 0.12840743362903595\n",
      "epoch 144:\n",
      "Loss train 0.003027787074446678 val 0.12778864800930023\n",
      "epoch 145:\n",
      "Loss train 0.0031064900700002907 val 0.12305942177772522\n",
      "epoch 146:\n",
      "Loss train 0.0030676017347723247 val 0.12653641402721405\n",
      "epoch 147:\n",
      "Loss train 0.003178290281444788 val 0.12706400454044342\n",
      "epoch 148:\n",
      "Loss train 0.0031496486011892556 val 0.12228589504957199\n",
      "epoch 149:\n",
      "Loss train 0.003069801153615117 val 0.12799355387687683\n",
      "epoch 150:\n",
      "Loss train 0.0031417035944759845 val 0.12921814620494843\n",
      "epoch 151:\n",
      "Loss train 0.003125243516638875 val 0.1269429475069046\n",
      "epoch 152:\n",
      "Loss train 0.002993451142683625 val 0.12541218101978302\n",
      "epoch 153:\n",
      "Loss train 0.0031016870997846125 val 0.12570920586585999\n",
      "epoch 154:\n",
      "Loss train 0.0030845737773925064 val 0.1308542639017105\n",
      "epoch 155:\n",
      "Loss train 0.003040947748348117 val 0.12944234907627106\n",
      "epoch 156:\n",
      "Loss train 0.003084807675331831 val 0.128633514046669\n",
      "epoch 157:\n",
      "Loss train 0.003101594749838114 val 0.12880480289459229\n",
      "epoch 158:\n",
      "Loss train 0.002982818542048335 val 0.12861457467079163\n",
      "epoch 159:\n",
      "Loss train 0.0030859970692545175 val 0.13145500421524048\n",
      "epoch 160:\n",
      "Loss train 0.0030398696307092905 val 0.12682129442691803\n",
      "epoch 161:\n",
      "Loss train 0.0030057625453919173 val 0.12340162694454193\n",
      "epoch 162:\n",
      "Loss train 0.002978948561474681 val 0.13041570782661438\n",
      "epoch 163:\n",
      "Loss train 0.0030389463398605584 val 0.13355101644992828\n",
      "epoch 164:\n",
      "Loss train 0.0029637912455946205 val 0.12754853069782257\n",
      "epoch 165:\n",
      "Loss train 0.002922932133078575 val 0.13360539078712463\n",
      "epoch 166:\n",
      "Loss train 0.0030139876063913105 val 0.13076290488243103\n",
      "epoch 167:\n",
      "Loss train 0.003023782445117831 val 0.133300319314003\n",
      "epoch 168:\n",
      "Loss train 0.0030842670146375897 val 0.127372145652771\n",
      "epoch 169:\n",
      "Loss train 0.0029527689199894667 val 0.1360113024711609\n",
      "epoch 170:\n",
      "Loss train 0.0029810970388352872 val 0.12841276824474335\n",
      "epoch 171:\n",
      "Loss train 0.0029522582609206437 val 0.1291189193725586\n",
      "epoch 172:\n",
      "Loss train 0.00303327252343297 val 0.1342865526676178\n",
      "epoch 173:\n",
      "Loss train 0.0029818975124508143 val 0.12549011409282684\n",
      "epoch 174:\n",
      "Loss train 0.0028806703090667723 val 0.13192008435726166\n",
      "epoch 175:\n",
      "Loss train 0.002921741396188736 val 0.1328514665365219\n",
      "epoch 176:\n",
      "Loss train 0.0029995118137449025 val 0.1316288411617279\n",
      "epoch 177:\n",
      "Loss train 0.00298128654435277 val 0.12469224631786346\n",
      "epoch 178:\n",
      "Loss train 0.0030829297713935375 val 0.13240928947925568\n",
      "epoch 179:\n",
      "Loss train 0.002860813561826944 val 0.13406437635421753\n",
      "epoch 180:\n",
      "Loss train 0.0029914763923734426 val 0.12846225500106812\n",
      "epoch 181:\n",
      "Loss train 0.002903584947809577 val 0.125743567943573\n",
      "epoch 182:\n",
      "Loss train 0.00303375631198287 val 0.12941914796829224\n",
      "epoch 183:\n",
      "Loss train 0.003000727839767933 val 0.12722201645374298\n",
      "epoch 184:\n",
      "Loss train 0.0029684692583978175 val 0.1294468641281128\n",
      "epoch 185:\n",
      "Loss train 0.002983653340488672 val 0.13417166471481323\n",
      "epoch 186:\n",
      "Loss train 0.0030000670282170176 val 0.12696236371994019\n",
      "epoch 187:\n",
      "Loss train 0.002959648320451379 val 0.1265576183795929\n",
      "epoch 188:\n",
      "Loss train 0.002907277898862958 val 0.12962333858013153\n",
      "epoch 189:\n",
      "Loss train 0.002970194974914193 val 0.12937773764133453\n",
      "epoch 190:\n",
      "Loss train 0.00303345999866724 val 0.13181690871715546\n",
      "epoch 191:\n",
      "Loss train 0.002949540100991726 val 0.13613438606262207\n",
      "epoch 192:\n",
      "Loss train 0.0028937564305961134 val 0.130547434091568\n",
      "epoch 193:\n",
      "Loss train 0.003013874849304557 val 0.1386415958404541\n",
      "epoch 194:\n",
      "Loss train 0.0029737412706017496 val 0.13219256699085236\n",
      "epoch 195:\n",
      "Loss train 0.0029007311630994083 val 0.12783528864383698\n",
      "epoch 196:\n",
      "Loss train 0.0029429307747632265 val 0.13711975514888763\n",
      "epoch 197:\n",
      "Loss train 0.0028914517238736153 val 0.1291017383337021\n",
      "epoch 198:\n",
      "Loss train 0.0029359334167093037 val 0.13098344206809998\n",
      "epoch 199:\n",
      "Loss train 0.002939266562461853 val 0.13432738184928894\n",
      "epoch 200:\n",
      "Loss train 0.002904185326769948 val 0.13311052322387695\n",
      "epoch 201:\n",
      "Loss train 0.0028788228295743463 val 0.1286371946334839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:44<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 202:\n",
      "Loss train 0.003012100614607334 val 0.13072076439857483\n",
      "epoch 203:\n",
      "Loss train 0.0029199567921459673 val 0.13456125557422638\n",
      "epoch 204:\n",
      "Loss train 0.0030235277619212867 val 0.13888585567474365\n",
      "epoch 205:\n",
      "Loss train 0.002998426027595997 val 0.13081631064414978\n",
      "epoch 206:\n",
      "Loss train 0.0028383654933422805 val 0.1327795535326004\n",
      "epoch 207:\n",
      "Loss train 0.00287128964997828 val 0.12915174663066864\n",
      "epoch 208:\n",
      "Loss train 0.00283648444339633 val 0.13492359220981598\n",
      "epoch 209:\n",
      "Loss train 0.002935904048383236 val 0.12963756918907166\n",
      "epoch 210:\n",
      "Loss train 0.002960858955979347 val 0.13095463812351227\n",
      "epoch 211:\n",
      "Loss train 0.0029254936017096044 val 0.12851200997829437\n",
      "epoch 212:\n",
      "Loss train 0.002930965529754758 val 0.13247685134410858\n",
      "epoch 213:\n",
      "Loss train 0.0029447086434811354 val 0.13306021690368652\n",
      "epoch 214:\n",
      "Loss train 0.00293585417047143 val 0.13165943324565887\n",
      "epoch 215:\n",
      "Loss train 0.0028458692599087954 val 0.1285165250301361\n",
      "epoch 216:\n",
      "Loss train 0.0029023040682077408 val 0.13146722316741943\n",
      "epoch 217:\n",
      "Loss train 0.002858294077217579 val 0.13117970526218414\n",
      "epoch 218:\n",
      "Loss train 0.0029455216787755488 val 0.13309074938297272\n",
      "epoch 219:\n",
      "Loss train 0.0028886962048709393 val 0.13329355418682098\n",
      "epoch 220:\n",
      "Loss train 0.0028796297814697026 val 0.13143429160118103\n",
      "epoch 221:\n",
      "Loss train 0.002805274996906519 val 0.12681445479393005\n",
      "epoch 222:\n",
      "Loss train 0.002902807291597128 val 0.13316698372364044\n",
      "epoch 223:\n",
      "Loss train 0.002808799000456929 val 0.13484223186969757\n",
      "epoch 224:\n",
      "Loss train 0.0029493618719279766 val 0.13097567856311798\n",
      "epoch 225:\n",
      "Loss train 0.0028574997819960117 val 0.1342441886663437\n",
      "epoch 226:\n",
      "Loss train 0.0028635646812617778 val 0.1331675499677658\n",
      "epoch 227:\n",
      "Loss train 0.0029086926542222498 val 0.13217152655124664\n",
      "epoch 228:\n",
      "Loss train 0.0027969099283218384 val 0.1261393427848816\n",
      "epoch 229:\n",
      "Loss train 0.002933836406096816 val 0.12940140068531036\n",
      "epoch 230:\n",
      "Loss train 0.002822491008788347 val 0.13208641111850739\n",
      "epoch 231:\n",
      "Loss train 0.002820275105535984 val 0.13683609664440155\n",
      "epoch 232:\n",
      "Loss train 0.0029280337020754815 val 0.13206136226654053\n",
      "epoch 233:\n",
      "Loss train 0.0029132630582898856 val 0.13581816852092743\n",
      "epoch 234:\n",
      "Loss train 0.002870058925822377 val 0.13056856393814087\n",
      "epoch 235:\n",
      "Loss train 0.002798472447320819 val 0.1345321238040924\n",
      "epoch 236:\n",
      "Loss train 0.0027347025759518148 val 0.13318169116973877\n",
      "epoch 237:\n",
      "Loss train 0.0029026666656136514 val 0.13696640729904175\n",
      "epoch 238:\n",
      "Loss train 0.0028442760445177556 val 0.128219336271286\n",
      "epoch 239:\n",
      "Loss train 0.002926323801279068 val 0.12614065408706665\n",
      "epoch 240:\n",
      "Loss train 0.002893343672156334 val 0.13378003239631653\n",
      "epoch 241:\n",
      "Loss train 0.002846488509327173 val 0.13355034589767456\n",
      "epoch 242:\n",
      "Loss train 0.0029200061317533254 val 0.13382604718208313\n",
      "epoch 243:\n",
      "Loss train 0.002890299145132303 val 0.13051488995552063\n",
      "epoch 244:\n",
      "Loss train 0.002898676345124841 val 0.1305178850889206\n",
      "epoch 245:\n",
      "Loss train 0.002818763818591833 val 0.13244205713272095\n",
      "epoch 246:\n",
      "Loss train 0.0028421458750963213 val 0.13665413856506348\n",
      "epoch 247:\n",
      "Loss train 0.0028350791949778797 val 0.1385631412267685\n",
      "epoch 248:\n",
      "Loss train 0.002834435248747468 val 0.13796091079711914\n",
      "epoch 249:\n",
      "Loss train 0.002844918441027403 val 0.13342906534671783\n",
      "epoch 250:\n",
      "Loss train 0.002824207466095686 val 0.13918286561965942\n",
      "epoch 251:\n",
      "Loss train 0.0029381172228604554 val 0.1362856775522232\n",
      "epoch 252:\n",
      "Loss train 0.0028412940967828034 val 0.13713712990283966\n",
      "epoch 253:\n",
      "Loss train 0.002789500843733549 val 0.13589058816432953\n",
      "epoch 254:\n",
      "Loss train 0.002875531408935785 val 0.13356122374534607\n",
      "epoch 255:\n",
      "Loss train 0.0028116478975862263 val 0.13765573501586914\n",
      "epoch 256:\n",
      "Loss train 0.0028676852621138094 val 0.14059744775295258\n",
      "epoch 257:\n",
      "Loss train 0.0027337683206424116 val 0.13632924854755402\n",
      "epoch 258:\n",
      "Loss train 0.002804052347317338 val 0.13661019504070282\n",
      "epoch 259:\n",
      "Loss train 0.0028643269147723915 val 0.1364966481924057\n",
      "epoch 260:\n",
      "Loss train 0.002846926020458341 val 0.14099860191345215\n",
      "epoch 261:\n",
      "Loss train 0.002824681700207293 val 0.1383081078529358\n",
      "epoch 262:\n",
      "Loss train 0.0028292051730677485 val 0.13402415812015533\n",
      "epoch 263:\n",
      "Loss train 0.0028203177843242884 val 0.1410530060529709\n",
      "epoch 264:\n",
      "Loss train 0.0028059815671294926 val 0.12752753496170044\n",
      "epoch 265:\n",
      "Loss train 0.0028393254689872264 val 0.1360631138086319\n",
      "epoch 266:\n",
      "Loss train 0.0028760768491774796 val 0.131120502948761\n",
      "epoch 267:\n",
      "Loss train 0.0027689382173120974 val 0.1307809054851532\n",
      "epoch 268:\n",
      "Loss train 0.0027965900376439095 val 0.14012254774570465\n",
      "epoch 269:\n",
      "Loss train 0.002776865703985095 val 0.1378743201494217\n",
      "epoch 270:\n",
      "Loss train 0.0027820179853588344 val 0.13779833912849426\n",
      "epoch 271:\n",
      "Loss train 0.002799460940063 val 0.146283358335495\n",
      "epoch 272:\n",
      "Loss train 0.0028453653510659934 val 0.1367601603269577\n",
      "epoch 273:\n",
      "Loss train 0.00274762250110507 val 0.13308851420879364\n",
      "epoch 274:\n",
      "Loss train 0.00288399900496006 val 0.13774272799491882\n",
      "epoch 275:\n",
      "Loss train 0.0027655826155096293 val 0.13535726070404053\n",
      "epoch 276:\n",
      "Loss train 0.0027353909239172937 val 0.1314016729593277\n",
      "epoch 277:\n",
      "Loss train 0.002894493056461215 val 0.13253068923950195\n",
      "epoch 278:\n",
      "Loss train 0.0028592847622931003 val 0.13527533411979675\n",
      "epoch 279:\n",
      "Loss train 0.002811422483995557 val 0.13484500348567963\n",
      "epoch 280:\n",
      "Loss train 0.0028495793454349043 val 0.13467714190483093\n",
      "epoch 281:\n",
      "Loss train 0.0028173168711364267 val 0.13406778872013092\n",
      "epoch 282:\n",
      "Loss train 0.002745595868676901 val 0.1326998621225357\n",
      "epoch 283:\n",
      "Loss train 0.0028545593824237585 val 0.13051344454288483\n",
      "epoch 284:\n",
      "Loss train 0.002799979681149125 val 0.1382211297750473\n",
      "epoch 285:\n",
      "Loss train 0.0027548586092889307 val 0.13185718655586243\n",
      "epoch 286:\n",
      "Loss train 0.0027609206084162 val 0.13539691269397736\n",
      "epoch 287:\n",
      "Loss train 0.002757450794801116 val 0.1310708224773407\n",
      "epoch 288:\n",
      "Loss train 0.0028289448749274017 val 0.13409796357154846\n",
      "epoch 289:\n",
      "Loss train 0.002801994316279888 val 0.1291477531194687\n",
      "epoch 290:\n",
      "Loss train 0.0028393197897821666 val 0.1373620331287384\n",
      "epoch 291:\n",
      "Loss train 0.0027363311778753996 val 0.13614928722381592\n",
      "epoch 292:\n",
      "Loss train 0.0027938185669481755 val 0.13741962611675262\n",
      "epoch 293:\n",
      "Loss train 0.0027635273225605486 val 0.13533265888690948\n",
      "epoch 294:\n",
      "Loss train 0.002671718208119273 val 0.13000069558620453\n",
      "epoch 295:\n",
      "Loss train 0.0028094270024448635 val 0.13504119217395782\n",
      "epoch 296:\n",
      "Loss train 0.0028929902017116546 val 0.13114194571971893\n",
      "epoch 297:\n",
      "Loss train 0.0027653688713908196 val 0.13263572752475739\n",
      "epoch 298:\n",
      "Loss train 0.002755386296659708 val 0.14013399183750153\n",
      "epoch 299:\n",
      "Loss train 0.0027751726880669595 val 0.13717974722385406\n",
      "epoch 300:\n",
      "Loss train 0.002785097881220281 val 0.13218384981155396\n",
      "epoch 301:\n",
      "Loss train 0.0027323578987270593 val 0.13774916529655457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:42<00:00, 23.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 302:\n",
      "Loss train 0.002836126321926713 val 0.13615280389785767\n",
      "epoch 303:\n",
      "Loss train 0.002726882940158248 val 0.133378803730011\n",
      "epoch 304:\n",
      "Loss train 0.002822103740647435 val 0.13866525888442993\n",
      "epoch 305:\n",
      "Loss train 0.0027229377180337908 val 0.1384442299604416\n",
      "epoch 306:\n",
      "Loss train 0.002718561850488186 val 0.1359541416168213\n",
      "epoch 307:\n",
      "Loss train 0.002691067675128579 val 0.14046527445316315\n",
      "epoch 308:\n",
      "Loss train 0.0027388985697180033 val 0.14290377497673035\n",
      "epoch 309:\n",
      "Loss train 1.6332069532480091 val 0.134016752243042\n",
      "epoch 310:\n",
      "Loss train 0.002672427276149392 val 0.13807179033756256\n",
      "epoch 311:\n",
      "Loss train 0.0028018085695803168 val 0.13587996363639832\n",
      "epoch 312:\n",
      "Loss train 0.002656214131042361 val 0.13116583228111267\n",
      "epoch 313:\n",
      "Loss train 0.002662895732559264 val 0.13699206709861755\n",
      "epoch 314:\n",
      "Loss train 0.0027674139756709336 val 0.14225322008132935\n",
      "epoch 315:\n",
      "Loss train 0.002672420619055629 val 0.14050118625164032\n",
      "epoch 316:\n",
      "Loss train 0.00270183583162725 val 0.1346438229084015\n",
      "epoch 317:\n",
      "Loss train 0.0026494160518050194 val 0.13772037625312805\n",
      "epoch 318:\n",
      "Loss train 0.002703702472150326 val 0.1392357051372528\n",
      "epoch 319:\n",
      "Loss train 0.002705564663745463 val 0.13247473537921906\n",
      "epoch 320:\n",
      "Loss train 0.0027184920813888313 val 0.13949470221996307\n",
      "epoch 321:\n",
      "Loss train 0.00271293892711401 val 0.13575173914432526\n",
      "epoch 322:\n",
      "Loss train 0.0026262148804962636 val 0.13872428238391876\n",
      "epoch 323:\n",
      "Loss train 0.0026143562253564596 val 0.14036035537719727\n",
      "epoch 324:\n",
      "Loss train 0.002667710063047707 val 0.13011953234672546\n",
      "epoch 325:\n",
      "Loss train 0.0026746290735900403 val 0.13765037059783936\n",
      "epoch 326:\n",
      "Loss train 0.0026942398864775894 val 0.13634119927883148\n",
      "epoch 327:\n",
      "Loss train 0.00267903695628047 val 0.1280345767736435\n",
      "epoch 328:\n",
      "Loss train 0.002598255282267928 val 0.13204443454742432\n",
      "epoch 329:\n",
      "Loss train 0.0026917622527107598 val 0.13730573654174805\n",
      "epoch 330:\n",
      "Loss train 0.0027188704274594784 val 0.13733422756195068\n",
      "epoch 331:\n",
      "Loss train 0.00269389527477324 val 0.13543696701526642\n",
      "epoch 332:\n",
      "Loss train 0.0026225691102445127 val 0.1389337033033371\n",
      "epoch 333:\n",
      "Loss train 0.0026990760331973432 val 0.13789138197898865\n",
      "epoch 334:\n",
      "Loss train 0.002648120688274503 val 0.1398317515850067\n",
      "epoch 335:\n",
      "Loss train 0.0026829726658761503 val 0.13440008461475372\n",
      "epoch 336:\n",
      "Loss train 0.0027448884267359972 val 0.13794054090976715\n",
      "epoch 337:\n",
      "Loss train 0.0026731881219893695 val 0.1376355141401291\n",
      "epoch 338:\n",
      "Loss train 0.0026663502529263496 val 0.13151191174983978\n",
      "epoch 339:\n",
      "Loss train 0.0027334608919918537 val 0.13644574582576752\n",
      "epoch 340:\n",
      "Loss train 0.002668131177313626 val 0.13964413106441498\n",
      "epoch 341:\n",
      "Loss train 0.0026478128861635924 val 0.1325751692056656\n",
      "epoch 342:\n",
      "Loss train 0.00266409504879266 val 0.13857121765613556\n",
      "epoch 343:\n",
      "Loss train 0.0026727564409375192 val 0.13683652877807617\n",
      "epoch 344:\n",
      "Loss train 0.002689112877473235 val 0.13606935739517212\n",
      "epoch 345:\n",
      "Loss train 0.002611891144886613 val 0.13904598355293274\n",
      "epoch 346:\n",
      "Loss train 0.002577282143756747 val 0.1320272535085678\n",
      "epoch 347:\n",
      "Loss train 0.002549893533810973 val 0.1412409543991089\n",
      "epoch 348:\n",
      "Loss train 0.002667555449530482 val 0.1390092372894287\n",
      "epoch 349:\n",
      "Loss train 0.0026642162054777143 val 0.14112509787082672\n",
      "epoch 350:\n",
      "Loss train 0.002563487043604255 val 0.1393250674009323\n",
      "epoch 351:\n",
      "Loss train 0.0027230163365602493 val 0.1363317221403122\n",
      "epoch 352:\n",
      "Loss train 0.0026806150339543817 val 0.13685829937458038\n",
      "epoch 353:\n",
      "Loss train 0.0026952038630843164 val 0.1366606205701828\n",
      "epoch 354:\n",
      "Loss train 0.002700122794136405 val 0.1340644359588623\n",
      "epoch 355:\n",
      "Loss train 0.0026109893564134836 val 0.13493739068508148\n",
      "epoch 356:\n",
      "Loss train 0.0026358203152194617 val 0.13698431849479675\n",
      "epoch 357:\n",
      "Loss train 0.002647589328698814 val 0.13850037753582\n",
      "epoch 358:\n",
      "Loss train 0.0026048238184303045 val 0.13668808341026306\n",
      "epoch 359:\n",
      "Loss train 0.0026487973909825086 val 0.13027381896972656\n",
      "epoch 360:\n",
      "Loss train 0.0026158650442957877 val 0.1369047462940216\n",
      "epoch 361:\n",
      "Loss train 0.0027351907808333634 val 0.13512542843818665\n",
      "epoch 362:\n",
      "Loss train 0.002634299362078309 val 0.13301755487918854\n",
      "epoch 363:\n",
      "Loss train 0.0026253663320094348 val 0.1392345428466797\n",
      "epoch 364:\n",
      "Loss train 0.002616461630910635 val 0.1386430710554123\n",
      "epoch 365:\n",
      "Loss train 0.002580486914142966 val 0.13525061309337616\n",
      "epoch 366:\n",
      "Loss train 0.002596351481974125 val 0.1354505717754364\n",
      "epoch 367:\n",
      "Loss train 0.0026093707950785756 val 0.13486309349536896\n",
      "epoch 368:\n",
      "Loss train 0.00262719258852303 val 0.1346319615840912\n",
      "epoch 369:\n",
      "Loss train 0.0026421988178044558 val 0.1325799971818924\n",
      "epoch 370:\n",
      "Loss train 0.002620809234678745 val 0.1362667679786682\n",
      "epoch 371:\n",
      "Loss train 0.0025390519257634876 val 0.13751114904880524\n",
      "epoch 372:\n",
      "Loss train 0.0025653572734445334 val 0.1373477429151535\n",
      "epoch 373:\n",
      "Loss train 0.002524845665320754 val 0.14315371215343475\n",
      "epoch 374:\n",
      "Loss train 0.002538802512921393 val 0.13912098109722137\n",
      "epoch 375:\n",
      "Loss train 0.0025208521485328675 val 0.1454220712184906\n",
      "epoch 376:\n",
      "Loss train 0.0025847775544971226 val 0.1394948810338974\n",
      "epoch 377:\n",
      "Loss train 0.0025258413795381783 val 0.13472293317317963\n",
      "epoch 378:\n",
      "Loss train 0.0026371298246085644 val 0.13605298101902008\n",
      "epoch 379:\n",
      "Loss train 0.0025552526134997606 val 0.13740649819374084\n",
      "epoch 380:\n",
      "Loss train 0.0026333189103752374 val 0.13651937246322632\n",
      "epoch 381:\n",
      "Loss train 0.0025321559561416506 val 0.1310453563928604\n",
      "epoch 382:\n",
      "Loss train 0.002549376191571355 val 0.14014297723770142\n",
      "epoch 383:\n",
      "Loss train 0.0025876363143324853 val 0.13260473310947418\n",
      "epoch 384:\n",
      "Loss train 0.002497329080477357 val 0.14053015410900116\n",
      "epoch 385:\n",
      "Loss train 0.002561786605045199 val 0.13692139089107513\n",
      "epoch 386:\n",
      "Loss train 0.0026177618876099588 val 0.1360042244195938\n",
      "epoch 387:\n",
      "Loss train 0.002579606270417571 val 0.13812857866287231\n",
      "epoch 388:\n",
      "Loss train 0.0025422283466905353 val 0.14294371008872986\n",
      "epoch 389:\n",
      "Loss train 0.0024956398736685516 val 0.13576030731201172\n",
      "epoch 390:\n",
      "Loss train 0.002518387106247246 val 0.14732208847999573\n",
      "epoch 391:\n",
      "Loss train 0.0026177308652549983 val 0.14292222261428833\n",
      "epoch 392:\n",
      "Loss train 0.0024917571283876896 val 0.1409578025341034\n",
      "epoch 393:\n",
      "Loss train 0.0025240543279796837 val 0.1462540179491043\n",
      "epoch 394:\n",
      "Loss train 0.002552317002788186 val 0.13576655089855194\n",
      "epoch 395:\n",
      "Loss train 0.0025318974554538725 val 0.14278747141361237\n",
      "epoch 396:\n",
      "Loss train 0.0025345284063369037 val 0.1412140280008316\n",
      "epoch 397:\n",
      "Loss train 0.002485665446147323 val 0.13700392842292786\n",
      "epoch 398:\n",
      "Loss train 0.0024252674505114554 val 0.14625532925128937\n",
      "epoch 399:\n",
      "Loss train 0.0024956618417054416 val 0.13539403676986694\n",
      "epoch 400:\n",
      "Loss train 0.0024055385049432517 val 0.14207705855369568\n",
      "epoch 401:\n",
      "Loss train 0.002453155893832445 val 0.13938012719154358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 402:\n",
      "Loss train 0.002596244404092431 val 0.14195367693901062\n",
      "epoch 403:\n",
      "Loss train 0.0024650017712265254 val 0.1410205066204071\n",
      "epoch 404:\n",
      "Loss train 0.002473275063559413 val 0.14689762890338898\n",
      "epoch 405:\n",
      "Loss train 0.002541146144270897 val 0.14273948967456818\n",
      "epoch 406:\n",
      "Loss train 0.0025177181623876094 val 0.13828735053539276\n",
      "epoch 407:\n",
      "Loss train 0.0025561963301151992 val 0.14835837483406067\n",
      "epoch 408:\n",
      "Loss train 0.002453950550407171 val 0.1449754685163498\n",
      "epoch 409:\n",
      "Loss train 0.002487912547774613 val 0.1398288607597351\n",
      "epoch 410:\n",
      "Loss train 0.002473792776465416 val 0.14071331918239594\n",
      "epoch 411:\n",
      "Loss train 0.0024648985732346773 val 0.13863202929496765\n",
      "epoch 412:\n",
      "Loss train 0.0025293112201616166 val 0.1451311707496643\n",
      "epoch 413:\n",
      "Loss train 0.0024540280252695083 val 0.140900656580925\n",
      "epoch 414:\n",
      "Loss train 0.0023843123503029346 val 0.1347668319940567\n",
      "epoch 415:\n",
      "Loss train 0.0023760896315798163 val 0.13788148760795593\n",
      "epoch 416:\n",
      "Loss train 0.0025597570072859524 val 0.14393338561058044\n",
      "epoch 417:\n",
      "Loss train 0.0025096910148859025 val 0.13526371121406555\n",
      "epoch 418:\n",
      "Loss train 0.002478449512273073 val 0.13487599790096283\n",
      "epoch 419:\n",
      "Loss train 0.002394342191517353 val 0.14625220000743866\n",
      "epoch 420:\n",
      "Loss train 0.0024891519816592337 val 0.1436121165752411\n",
      "epoch 421:\n",
      "Loss train 0.0024061664510518312 val 0.14853733777999878\n",
      "epoch 422:\n",
      "Loss train 0.0024215445360168814 val 0.13697759807109833\n",
      "epoch 423:\n",
      "Loss train 0.002467045368626714 val 0.14032742381095886\n",
      "epoch 424:\n",
      "Loss train 0.0025414192248135806 val 0.14041677117347717\n",
      "epoch 425:\n",
      "Loss train 0.0024397065080702307 val 0.14168491959571838\n",
      "epoch 426:\n",
      "Loss train 0.0023858650699257852 val 0.1395508199930191\n",
      "epoch 427:\n",
      "Loss train 0.00241272259876132 val 0.14261573553085327\n",
      "epoch 428:\n",
      "Loss train 0.002399312263354659 val 0.1449994593858719\n",
      "epoch 429:\n",
      "Loss train 0.002417474195361137 val 0.14577217400074005\n",
      "epoch 430:\n",
      "Loss train 0.002424440497532487 val 0.13673029839992523\n",
      "epoch 431:\n",
      "Loss train 0.0025164423733949662 val 0.14553631842136383\n",
      "epoch 432:\n",
      "Loss train 0.002384092398919165 val 0.1436062753200531\n",
      "epoch 433:\n",
      "Loss train 0.002490656024776399 val 0.1442318856716156\n",
      "epoch 434:\n",
      "Loss train 0.002393674835562706 val 0.13663434982299805\n",
      "epoch 435:\n",
      "Loss train 0.0024326187670230866 val 0.14482955634593964\n",
      "epoch 436:\n",
      "Loss train 0.002352646172977984 val 0.1450669914484024\n",
      "epoch 437:\n",
      "Loss train 0.002343280976638198 val 0.14711327850818634\n",
      "epoch 438:\n",
      "Loss train 0.002389995165169239 val 0.13804329931735992\n",
      "epoch 439:\n",
      "Loss train 0.002397097290493548 val 0.13806530833244324\n",
      "epoch 440:\n",
      "Loss train 0.0024156343061476947 val 0.14502179622650146\n",
      "epoch 441:\n",
      "Loss train 0.0023985118623822926 val 0.14809271693229675\n",
      "epoch 442:\n",
      "Loss train 0.002391923570074141 val 0.13830649852752686\n",
      "epoch 443:\n",
      "Loss train 0.0023812498599290848 val 0.14066766202449799\n",
      "epoch 444:\n",
      "Loss train 0.00236072869412601 val 0.13776132464408875\n",
      "epoch 445:\n",
      "Loss train 0.002348322218284011 val 0.14378447830677032\n",
      "epoch 446:\n",
      "Loss train 0.002323930078186095 val 0.13783597946166992\n",
      "epoch 447:\n",
      "Loss train 0.002379831469617784 val 0.1419886350631714\n",
      "epoch 448:\n",
      "Loss train 0.0023960806746035814 val 0.1471826434135437\n",
      "epoch 449:\n",
      "Loss train 0.0023612541183829308 val 0.14279510080814362\n",
      "epoch 450:\n",
      "Loss train 0.0023974021952599288 val 0.142191082239151\n",
      "epoch 451:\n",
      "Loss train 0.0024303393326699733 val 0.14483773708343506\n",
      "epoch 452:\n",
      "Loss train 0.0024062664434313773 val 0.14532442390918732\n",
      "epoch 453:\n",
      "Loss train 0.0023638393096625807 val 0.1460149586200714\n",
      "epoch 454:\n",
      "Loss train 0.002372727731242776 val 0.14983931183815002\n",
      "epoch 455:\n",
      "Loss train 0.002420579060912132 val 0.14195004105567932\n",
      "epoch 456:\n",
      "Loss train 0.002323639988899231 val 0.14555606245994568\n",
      "epoch 457:\n",
      "Loss train 0.0023256261497735975 val 0.14288544654846191\n",
      "epoch 458:\n",
      "Loss train 0.0023933934941887855 val 0.14736497402191162\n",
      "epoch 459:\n",
      "Loss train 0.002407588364556432 val 0.13663774728775024\n",
      "epoch 460:\n",
      "Loss train 0.0023049375507980586 val 0.1504906713962555\n",
      "epoch 461:\n",
      "Loss train 0.0023440356235951186 val 0.13884755969047546\n",
      "epoch 462:\n",
      "Loss train 0.0023346368549391626 val 0.15124186873435974\n",
      "epoch 463:\n",
      "Loss train 0.0023263051379472017 val 0.14262039959430695\n",
      "epoch 464:\n",
      "Loss train 0.0022379347756505013 val 0.14407594501972198\n",
      "epoch 465:\n",
      "Loss train 0.002413495605811477 val 0.14399190247058868\n",
      "epoch 466:\n",
      "Loss train 0.00234465519618243 val 0.1419534981250763\n",
      "epoch 467:\n",
      "Loss train 0.0023274255506694316 val 0.13902072608470917\n",
      "epoch 468:\n",
      "Loss train 0.002323453008197248 val 0.1399666666984558\n",
      "epoch 469:\n",
      "Loss train 0.0023423583786934613 val 0.14323857426643372\n",
      "epoch 470:\n",
      "Loss train 0.0023444907627999784 val 0.14074696600437164\n",
      "epoch 471:\n",
      "Loss train 0.0021922344900667666 val 0.14573045074939728\n",
      "epoch 472:\n",
      "Loss train 0.0022790066245943308 val 0.13683979213237762\n",
      "epoch 473:\n",
      "Loss train 0.00234988119173795 val 0.13576440513134003\n",
      "epoch 474:\n",
      "Loss train 0.0023079024674370883 val 0.13647796213626862\n",
      "epoch 475:\n",
      "Loss train 0.002350328057073057 val 0.14372508227825165\n",
      "epoch 476:\n",
      "Loss train 0.0022864687517285347 val 0.14455731213092804\n",
      "epoch 477:\n",
      "Loss train 0.002331356007605791 val 0.14265039563179016\n",
      "epoch 478:\n",
      "Loss train 0.0023459772216156125 val 0.14483682811260223\n",
      "epoch 479:\n",
      "Loss train 0.002265565815381706 val 0.13550694286823273\n",
      "epoch 480:\n",
      "Loss train 0.002312927186489105 val 0.1378331184387207\n",
      "epoch 481:\n",
      "Loss train 0.0022625962076708674 val 0.14233100414276123\n",
      "epoch 482:\n",
      "Loss train 0.002232398091815412 val 0.14014911651611328\n",
      "epoch 483:\n",
      "Loss train 0.0023251680750399828 val 0.140810027718544\n",
      "epoch 484:\n",
      "Loss train 0.002281094416975975 val 0.14166000485420227\n",
      "epoch 485:\n",
      "Loss train 0.0023248737081885337 val 0.13825833797454834\n",
      "epoch 486:\n",
      "Loss train 0.0022661200361326337 val 0.1431201845407486\n",
      "epoch 487:\n",
      "Loss train 0.0022824108554050327 val 0.14704132080078125\n",
      "epoch 488:\n",
      "Loss train 0.0022676553195342423 val 0.14268113672733307\n",
      "epoch 489:\n",
      "Loss train 0.0022269276324659586 val 0.1463751345872879\n",
      "epoch 490:\n",
      "Loss train 0.002271123306825757 val 0.136823371052742\n",
      "epoch 491:\n",
      "Loss train 0.002243672553449869 val 0.14671048521995544\n",
      "epoch 492:\n",
      "Loss train 0.002246199004352093 val 0.15023718774318695\n",
      "epoch 493:\n",
      "Loss train 0.002232537627220154 val 0.14775194227695465\n",
      "epoch 494:\n",
      "Loss train 0.0022267116494476794 val 0.14361166954040527\n",
      "epoch 495:\n",
      "Loss train 0.0021785371489822865 val 0.14208588004112244\n",
      "epoch 496:\n",
      "Loss train 0.002233816401101649 val 0.14750322699546814\n",
      "epoch 497:\n",
      "Loss train 0.0022664741557091475 val 0.13863041996955872\n",
      "epoch 498:\n",
      "Loss train 0.0021907597621902823 val 0.1472170352935791\n",
      "epoch 499:\n",
      "Loss train 0.0022842908315360544 val 0.1439882218837738\n",
      "epoch 500:\n",
      "Loss train 0.0023111872589215636 val 0.14069823920726776\n",
      "epoch 501:\n",
      "Loss train 0.0022097044540569186 val 0.1402430385351181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:40<00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 502:\n",
      "Loss train 0.0022654493041336537 val 0.13944584131240845\n",
      "epoch 503:\n",
      "Loss train 0.0023193620555102827 val 0.14306333661079407\n",
      "epoch 504:\n",
      "Loss train 0.0022076824894174934 val 0.1415918916463852\n",
      "epoch 505:\n",
      "Loss train 0.002264402598142624 val 0.14486399292945862\n",
      "epoch 506:\n",
      "Loss train 0.002302089028060436 val 0.13737307488918304\n",
      "epoch 507:\n",
      "Loss train 0.002247958027757704 val 0.14782369136810303\n",
      "epoch 508:\n",
      "Loss train 0.0022890255833044647 val 0.13814523816108704\n",
      "epoch 509:\n",
      "Loss train 0.002184304120019078 val 0.14578919112682343\n",
      "epoch 510:\n",
      "Loss train 0.002251549968495965 val 0.14321103692054749\n",
      "epoch 511:\n",
      "Loss train 0.002242999425157905 val 0.14044177532196045\n",
      "epoch 512:\n",
      "Loss train 0.0021890879552811386 val 0.14809174835681915\n",
      "epoch 513:\n",
      "Loss train 0.002211575559340417 val 0.14332067966461182\n",
      "epoch 514:\n",
      "Loss train 0.0023007832700386644 val 0.1493394821882248\n",
      "epoch 515:\n",
      "Loss train 0.0022483365619555117 val 0.14246854186058044\n",
      "epoch 516:\n",
      "Loss train 0.0022245914097875357 val 0.14145317673683167\n",
      "epoch 517:\n",
      "Loss train 0.002178905410692096 val 0.14564940333366394\n",
      "epoch 518:\n",
      "Loss train 0.0021867518676444886 val 0.14243976771831512\n",
      "epoch 519:\n",
      "Loss train 0.00213738766964525 val 0.14432960748672485\n",
      "epoch 520:\n",
      "Loss train 0.002157088034786284 val 0.1502617746591568\n",
      "epoch 521:\n",
      "Loss train 0.002159040158614516 val 0.15003825724124908\n",
      "epoch 522:\n",
      "Loss train 0.0022207976682111622 val 0.13760347664356232\n",
      "epoch 523:\n",
      "Loss train 0.0022038499750196934 val 0.13649138808250427\n",
      "epoch 524:\n",
      "Loss train 0.0021740601286292078 val 0.14043891429901123\n",
      "epoch 525:\n",
      "Loss train 0.0021580045837908983 val 0.14243334531784058\n",
      "epoch 526:\n",
      "Loss train 0.002178954040631652 val 0.14816048741340637\n",
      "epoch 527:\n",
      "Loss train 0.0021747908648103477 val 0.15053187310695648\n",
      "epoch 528:\n",
      "Loss train 0.0022178627960383894 val 0.14436569809913635\n",
      "epoch 529:\n",
      "Loss train 0.002116016977466643 val 0.1411408632993698\n",
      "epoch 530:\n",
      "Loss train 0.0022357564587146043 val 0.14353151619434357\n",
      "epoch 531:\n",
      "Loss train 3.0903231029994784 val 0.14623482525348663\n",
      "epoch 532:\n",
      "Loss train 0.33594520049355925 val 0.14793547987937927\n",
      "epoch 533:\n",
      "Loss train 0.0021940694525837898 val 0.14188455045223236\n",
      "epoch 534:\n",
      "Loss train 0.002135366616770625 val 0.1388654261827469\n",
      "epoch 535:\n",
      "Loss train 0.002167927024886012 val 0.15047940611839294\n",
      "epoch 536:\n",
      "Loss train 0.0021750964540988207 val 0.15314266085624695\n",
      "epoch 537:\n",
      "Loss train 0.0021872824244201184 val 0.14696156978607178\n",
      "epoch 538:\n",
      "Loss train 0.0021654371777549385 val 0.1475040465593338\n",
      "epoch 539:\n",
      "Loss train 0.002233715021982789 val 0.13686822354793549\n",
      "epoch 540:\n",
      "Loss train 0.0021439339593052865 val 0.1343366652727127\n",
      "epoch 541:\n",
      "Loss train 0.002173093382269144 val 0.14449214935302734\n",
      "epoch 542:\n",
      "Loss train 0.002228485525585711 val 0.14116764068603516\n",
      "epoch 543:\n",
      "Loss train 0.0021092250337824223 val 0.14081817865371704\n",
      "epoch 544:\n",
      "Loss train 0.002222747444175184 val 0.1443335860967636\n",
      "epoch 545:\n",
      "Loss train 0.0020430358676239847 val 0.14233039319515228\n",
      "epoch 546:\n",
      "Loss train 0.0021471015065908433 val 0.14812912046909332\n",
      "epoch 547:\n",
      "Loss train 0.002147070901468396 val 0.14308050274848938\n",
      "epoch 548:\n",
      "Loss train 0.0021648644935339688 val 0.14377854764461517\n",
      "epoch 549:\n",
      "Loss train 0.0020998211149126293 val 0.1398758888244629\n",
      "epoch 550:\n",
      "Loss train 0.0021116253761574626 val 0.1408175230026245\n",
      "epoch 551:\n",
      "Loss train 0.0021071628285571934 val 0.14436696469783783\n",
      "epoch 552:\n",
      "Loss train 0.0021826864713802934 val 0.14676281809806824\n",
      "epoch 553:\n",
      "Loss train 0.0021674690702930093 val 0.1429119110107422\n",
      "epoch 554:\n",
      "Loss train 0.0021473322650417685 val 0.14936718344688416\n",
      "epoch 555:\n",
      "Loss train 0.002155891572125256 val 0.14552955329418182\n",
      "epoch 556:\n",
      "Loss train 0.002088268154300749 val 0.14446362853050232\n",
      "epoch 557:\n",
      "Loss train 0.0021358445938676595 val 0.14649252593517303\n",
      "epoch 558:\n",
      "Loss train 0.0020619794633239506 val 0.14704960584640503\n",
      "epoch 559:\n",
      "Loss train 0.0020936510832980275 val 0.1414511650800705\n",
      "epoch 560:\n",
      "Loss train 0.0020784900514408946 val 0.1532176434993744\n",
      "epoch 561:\n",
      "Loss train 0.002108048434369266 val 0.14267563819885254\n",
      "epoch 562:\n",
      "Loss train 0.002096562153659761 val 0.14430426061153412\n",
      "epoch 563:\n",
      "Loss train 0.002168256338685751 val 0.14312340319156647\n",
      "epoch 564:\n",
      "Loss train 0.0020348769100382923 val 0.15238021314144135\n",
      "epoch 565:\n",
      "Loss train 0.0020757846338674427 val 0.14507928490638733\n",
      "epoch 566:\n",
      "Loss train 0.0020532559035345913 val 0.14726200699806213\n",
      "epoch 567:\n",
      "Loss train 0.002060307053849101 val 0.1453307867050171\n",
      "epoch 568:\n",
      "Loss train 0.002079527774825692 val 0.14973099529743195\n",
      "epoch 569:\n",
      "Loss train 0.002128230388276279 val 0.1433141529560089\n",
      "epoch 570:\n",
      "Loss train 0.0020794446934014558 val 0.14789819717407227\n",
      "epoch 571:\n",
      "Loss train 0.002092756163328886 val 0.1501287966966629\n",
      "epoch 572:\n",
      "Loss train 0.002054532150737941 val 0.14368844032287598\n",
      "epoch 573:\n",
      "Loss train 0.002174310320056975 val 0.14653541147708893\n",
      "epoch 574:\n",
      "Loss train 0.0020368119226768615 val 0.14946521818637848\n",
      "epoch 575:\n",
      "Loss train 0.002117589503526688 val 0.14379604160785675\n",
      "epoch 576:\n",
      "Loss train 0.002095801439136267 val 0.1395094394683838\n",
      "epoch 577:\n",
      "Loss train 0.002087974840775132 val 0.14556898176670074\n",
      "epoch 578:\n",
      "Loss train 0.002071688354946673 val 0.14582686126232147\n",
      "epoch 579:\n",
      "Loss train 0.0020563554223626854 val 0.1492813378572464\n",
      "epoch 580:\n",
      "Loss train 0.0020872097937390207 val 0.13899336755275726\n",
      "epoch 581:\n",
      "Loss train 0.002002305392175913 val 0.15181244909763336\n",
      "epoch 582:\n",
      "Loss train 0.0020371246077120304 val 0.1445678323507309\n",
      "epoch 583:\n",
      "Loss train 0.0020737380031496284 val 0.1435433179140091\n",
      "epoch 584:\n",
      "Loss train 0.002051618362776935 val 0.1439775973558426\n",
      "epoch 585:\n",
      "Loss train 0.002110363149084151 val 0.14656521379947662\n",
      "epoch 586:\n",
      "Loss train 0.0020174039071425794 val 0.14272841811180115\n",
      "epoch 587:\n",
      "Loss train 0.0020654457779601216 val 0.14227530360221863\n",
      "epoch 588:\n",
      "Loss train 0.0020311735086143015 val 0.15003328025341034\n",
      "epoch 589:\n",
      "Loss train 0.002064801392145455 val 0.14452798664569855\n",
      "epoch 590:\n",
      "Loss train 0.00205291963648051 val 0.14857105910778046\n",
      "epoch 591:\n",
      "Loss train 0.0020878850799053907 val 0.1519603431224823\n",
      "epoch 592:\n",
      "Loss train 0.0020139962090179324 val 0.14876951277256012\n",
      "epoch 593:\n",
      "Loss train 0.0020256080674007533 val 0.1493193805217743\n",
      "epoch 594:\n",
      "Loss train 0.0020588291846215727 val 0.1542188972234726\n",
      "epoch 595:\n",
      "Loss train 0.0019826383497565984 val 0.1447598785161972\n",
      "epoch 596:\n",
      "Loss train 0.002021368930116296 val 0.1421545445919037\n",
      "epoch 597:\n",
      "Loss train 0.002011264800094068 val 0.14977949857711792\n",
      "epoch 598:\n",
      "Loss train 0.002009411187842488 val 0.14777280390262604\n",
      "epoch 599:\n",
      "Loss train 0.0019996261466294525 val 0.15041618049144745\n",
      "epoch 600:\n",
      "Loss train 0.0020490817530080677 val 0.1497429609298706\n",
      "epoch 601:\n",
      "Loss train 0.0020271062348037956 val 0.1452181190252304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 602:\n",
      "Loss train 0.0019281487921252846 val 0.1486426144838333\n",
      "epoch 603:\n",
      "Loss train 0.002012133370153606 val 0.14816483855247498\n",
      "epoch 604:\n",
      "Loss train 0.0020105648450553415 val 0.1536964774131775\n",
      "epoch 605:\n",
      "Loss train 0.002013957227580249 val 0.14736483991146088\n",
      "epoch 606:\n",
      "Loss train 0.0020708226710557938 val 0.14823223650455475\n",
      "epoch 607:\n",
      "Loss train 0.0019476202744990586 val 0.15227270126342773\n",
      "epoch 608:\n",
      "Loss train 0.0019398686597123741 val 0.14290374517440796\n",
      "epoch 609:\n",
      "Loss train 0.0020496798902750017 val 0.14608699083328247\n",
      "epoch 610:\n",
      "Loss train 0.002083740313537419 val 0.15028737485408783\n",
      "epoch 611:\n",
      "Loss train 0.0019736134307459 val 0.1469837725162506\n",
      "epoch 612:\n",
      "Loss train 0.0019319475470110773 val 0.14882658421993256\n",
      "epoch 613:\n",
      "Loss train 0.0019712087251245974 val 0.15242347121238708\n",
      "epoch 614:\n",
      "Loss train 0.0019428011626005173 val 0.14364561438560486\n",
      "epoch 615:\n",
      "Loss train 0.0020233078468590974 val 0.13476337492465973\n",
      "epoch 616:\n",
      "Loss train 0.0020317101422697307 val 0.14475160837173462\n",
      "epoch 617:\n",
      "Loss train 0.002037635449320078 val 0.14189332723617554\n",
      "epoch 618:\n",
      "Loss train 0.002066597379744053 val 0.14799784123897552\n",
      "epoch 619:\n",
      "Loss train 0.001913863580673933 val 0.14592108130455017\n",
      "epoch 620:\n",
      "Loss train 0.0019390912689268589 val 0.14665135741233826\n",
      "epoch 621:\n",
      "Loss train 0.0019959740955382585 val 0.14421437680721283\n",
      "epoch 622:\n",
      "Loss train 0.002021630049683154 val 0.14734549820423126\n",
      "epoch 623:\n",
      "Loss train 0.002014232953079045 val 0.15029683709144592\n",
      "epoch 624:\n",
      "Loss train 0.001993492629379034 val 0.14584515988826752\n",
      "epoch 625:\n",
      "Loss train 0.001983471660874784 val 0.13932575285434723\n",
      "epoch 626:\n",
      "Loss train 0.0019441672461107374 val 0.1422702670097351\n",
      "epoch 627:\n",
      "Loss train 0.0020148107754066587 val 0.14191000163555145\n",
      "epoch 628:\n",
      "Loss train 0.00204423935059458 val 0.14377140998840332\n",
      "epoch 629:\n",
      "Loss train 0.001962168593890965 val 0.15017998218536377\n",
      "epoch 630:\n",
      "Loss train 0.0019312750445678831 val 0.1496165245771408\n",
      "epoch 631:\n",
      "Loss train 0.0019367635836824776 val 0.14870917797088623\n",
      "epoch 632:\n",
      "Loss train 0.0019635647982358933 val 0.1477499008178711\n",
      "epoch 633:\n",
      "Loss train 0.0019874486047774553 val 0.1373993456363678\n",
      "epoch 634:\n",
      "Loss train 0.002000309191644192 val 0.14654575288295746\n",
      "epoch 635:\n",
      "Loss train 0.001961230576969683 val 0.144193634390831\n",
      "epoch 636:\n",
      "Loss train 0.001970896948128939 val 0.14463140070438385\n",
      "epoch 637:\n",
      "Loss train 0.0019318678956478834 val 0.14713670313358307\n",
      "epoch 638:\n",
      "Loss train 0.0019556880556046963 val 0.14549869298934937\n",
      "epoch 639:\n",
      "Loss train 0.0018812049347907305 val 0.15424546599388123\n",
      "epoch 640:\n",
      "Loss train 0.0019264157498255373 val 0.1441706418991089\n",
      "epoch 641:\n",
      "Loss train 0.0019033123906701803 val 0.14688272774219513\n",
      "epoch 642:\n",
      "Loss train 0.0019216320319101214 val 0.14434199035167694\n",
      "epoch 643:\n",
      "Loss train 0.0018775236532092095 val 0.14410410821437836\n",
      "epoch 644:\n",
      "Loss train 0.0019040741547942162 val 0.14782261848449707\n",
      "epoch 645:\n",
      "Loss train 0.001894186694175005 val 0.14536522328853607\n",
      "epoch 646:\n",
      "Loss train 0.0018643773216754198 val 0.14247436821460724\n",
      "epoch 647:\n",
      "Loss train 0.0020042735319584608 val 0.14654844999313354\n",
      "epoch 648:\n",
      "Loss train 0.0019924158314242957 val 0.15200084447860718\n",
      "epoch 649:\n",
      "Loss train 0.001900487156584859 val 0.15014176070690155\n",
      "epoch 650:\n",
      "Loss train 0.0019297631848603487 val 0.15576571226119995\n",
      "epoch 651:\n",
      "Loss train 0.001950928892940283 val 0.1524476855993271\n",
      "epoch 652:\n",
      "Loss train 0.001920823646709323 val 0.1514982432126999\n",
      "epoch 653:\n",
      "Loss train 0.0018946411088109016 val 0.14381439983844757\n",
      "epoch 654:\n",
      "Loss train 0.0019265020443126558 val 0.14690299332141876\n",
      "epoch 655:\n",
      "Loss train 0.0019418455911800266 val 0.1463085263967514\n",
      "epoch 656:\n",
      "Loss train 0.0019527431437745691 val 0.14731334149837494\n",
      "epoch 657:\n",
      "Loss train 0.0018877094332128764 val 0.14281612634658813\n",
      "epoch 658:\n",
      "Loss train 0.001925047654658556 val 0.14511153101921082\n",
      "epoch 659:\n",
      "Loss train 0.0019178681578487157 val 0.15450260043144226\n",
      "epoch 660:\n",
      "Loss train 0.0019354454465210437 val 0.14634427428245544\n",
      "epoch 661:\n",
      "Loss train 0.0019395823953673244 val 0.15098482370376587\n",
      "epoch 662:\n",
      "Loss train 0.0020314821265637875 val 0.15246284008026123\n",
      "epoch 663:\n",
      "Loss train 0.001956751895137131 val 0.1481342762708664\n",
      "epoch 664:\n",
      "Loss train 0.001852299108169973 val 0.1471325010061264\n",
      "epoch 665:\n",
      "Loss train 0.001979642822407186 val 0.1438218355178833\n",
      "epoch 666:\n",
      "Loss train 0.0018951964313164353 val 0.1451282948255539\n",
      "epoch 667:\n",
      "Loss train 0.00192462158203125 val 0.15044890344142914\n",
      "epoch 668:\n",
      "Loss train 0.0019716719686985015 val 0.15193605422973633\n",
      "epoch 669:\n",
      "Loss train 0.0019068765072152018 val 0.15431107580661774\n",
      "epoch 670:\n",
      "Loss train 0.001865407415665686 val 0.14483480155467987\n",
      "epoch 671:\n",
      "Loss train 0.0018949194764718413 val 0.15155334770679474\n",
      "epoch 672:\n",
      "Loss train 0.0019066129373386503 val 0.1456051915884018\n",
      "epoch 673:\n",
      "Loss train 0.0018678108742460609 val 0.15149568021297455\n",
      "epoch 674:\n",
      "Loss train 0.0019464313145726919 val 0.14647290110588074\n",
      "epoch 675:\n",
      "Loss train 0.0019211757322773338 val 0.15525676310062408\n",
      "epoch 676:\n",
      "Loss train 0.0019143538298085332 val 0.15072698891162872\n",
      "epoch 677:\n",
      "Loss train 0.0018991025751456617 val 0.15080426633358002\n",
      "epoch 678:\n",
      "Loss train 0.001962483748793602 val 0.14848285913467407\n",
      "epoch 679:\n",
      "Loss train 0.001844782312400639 val 0.14832407236099243\n",
      "epoch 680:\n",
      "Loss train 0.0018661195412278176 val 0.14555723965168\n",
      "epoch 681:\n",
      "Loss train 0.0019016517456620931 val 0.15140590071678162\n",
      "epoch 682:\n",
      "Loss train 0.0018674965500831604 val 0.1464906483888626\n",
      "epoch 683:\n",
      "Loss train 0.0018667595591396093 val 0.15373480319976807\n",
      "epoch 684:\n",
      "Loss train 0.0018157526720315217 val 0.14617471396923065\n",
      "epoch 685:\n",
      "Loss train 0.0019403722574934364 val 0.14562714099884033\n",
      "epoch 686:\n",
      "Loss train 0.0018969175089150667 val 0.14590343832969666\n",
      "epoch 687:\n",
      "Loss train 0.0018285292768850923 val 0.14771302044391632\n",
      "epoch 688:\n",
      "Loss train 0.0018241632226854562 val 0.1506304293870926\n",
      "epoch 689:\n",
      "Loss train 0.0018662540037184953 val 0.1580776423215866\n",
      "epoch 690:\n",
      "Loss train 0.0018970012431964278 val 0.15317603945732117\n",
      "epoch 691:\n",
      "Loss train 0.001899481994099915 val 0.14194577932357788\n",
      "epoch 692:\n",
      "Loss train 1.0910042642839253 val 0.15268616378307343\n",
      "epoch 693:\n",
      "Loss train 0.001845280472189188 val 0.15302683413028717\n",
      "epoch 694:\n",
      "Loss train 0.0019273774102330208 val 0.14545495808124542\n",
      "epoch 695:\n",
      "Loss train 0.0018457868229597807 val 0.15152768790721893\n",
      "epoch 696:\n",
      "Loss train 0.0018712280848994852 val 0.15774942934513092\n",
      "epoch 697:\n",
      "Loss train 0.0018711044071242212 val 0.15227758884429932\n",
      "epoch 698:\n",
      "Loss train 0.0018547928696498275 val 0.1529051959514618\n",
      "epoch 699:\n",
      "Loss train 0.001887210720218718 val 0.15634571015834808\n",
      "epoch 700:\n",
      "Loss train 0.0017545686811208726 val 0.1594562530517578\n",
      "epoch 701:\n",
      "Loss train 0.001823766039684415 val 0.1519772857427597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 702:\n",
      "Loss train 0.0018780720625072719 val 0.14889372885227203\n",
      "epoch 703:\n",
      "Loss train 0.0018082437394186855 val 0.15052905678749084\n",
      "epoch 704:\n",
      "Loss train 0.20390807277988643 val 0.14602351188659668\n",
      "epoch 705:\n",
      "Loss train 0.0018700958341360092 val 0.14462022483348846\n",
      "epoch 706:\n",
      "Loss train 0.001897739932872355 val 0.15038061141967773\n",
      "epoch 707:\n",
      "Loss train 0.0018663711836561562 val 0.15377874672412872\n",
      "epoch 708:\n",
      "Loss train 0.001819675898179412 val 0.1479349434375763\n",
      "epoch 709:\n",
      "Loss train 0.001858652425929904 val 0.15154646337032318\n",
      "epoch 710:\n",
      "Loss train 0.0018654292086139322 val 0.15192177891731262\n",
      "epoch 711:\n",
      "Loss train 0.0018412195891141891 val 0.15523923933506012\n",
      "epoch 712:\n",
      "Loss train 0.0018521779524162411 val 0.14984703063964844\n",
      "epoch 713:\n",
      "Loss train 0.0017623327076435088 val 0.14952488243579865\n",
      "epoch 714:\n",
      "Loss train 0.0018070061523467302 val 0.1559743732213974\n",
      "epoch 715:\n",
      "Loss train 0.0018167939027771353 val 0.15186168253421783\n",
      "epoch 716:\n",
      "Loss train 0.0018246699776500463 val 0.1554962396621704\n",
      "epoch 717:\n",
      "Loss train 0.0018779889456927777 val 0.15360936522483826\n",
      "epoch 718:\n",
      "Loss train 0.0018603549832478165 val 0.15441946685314178\n",
      "epoch 719:\n",
      "Loss train 0.001833823316730559 val 0.1584172248840332\n",
      "epoch 720:\n",
      "Loss train 0.0018073505917564035 val 0.15234458446502686\n",
      "epoch 721:\n",
      "Loss train 0.001807376985438168 val 0.1498975306749344\n",
      "epoch 722:\n",
      "Loss train 0.0017907440606504679 val 0.1476721167564392\n",
      "epoch 723:\n",
      "Loss train 0.0018044608337804675 val 0.15283608436584473\n",
      "epoch 724:\n",
      "Loss train 0.0018180319890379905 val 0.15186141431331635\n",
      "epoch 725:\n",
      "Loss train 0.0017859623869881033 val 0.15501387417316437\n",
      "epoch 726:\n",
      "Loss train 0.0017661782670766115 val 0.15508690476417542\n",
      "epoch 727:\n",
      "Loss train 0.0018777329763397575 val 0.15224049985408783\n",
      "epoch 728:\n",
      "Loss train 0.0018624955881386995 val 0.15204620361328125\n",
      "epoch 729:\n",
      "Loss train 0.0017800617525354028 val 0.15447483956813812\n",
      "epoch 730:\n",
      "Loss train 0.001845005586743355 val 0.14605411887168884\n",
      "epoch 731:\n",
      "Loss train 0.0018474514810368418 val 0.15411792695522308\n",
      "epoch 732:\n",
      "Loss train 0.0018029222004115582 val 0.15795865654945374\n",
      "epoch 733:\n",
      "Loss train 0.0017833178453147411 val 0.1446525603532791\n",
      "epoch 734:\n",
      "Loss train 0.0018074751580134034 val 0.15966381132602692\n",
      "epoch 735:\n",
      "Loss train 0.0018422277579084039 val 0.1541977971792221\n",
      "epoch 736:\n",
      "Loss train 0.0018346693441271783 val 0.14596439898014069\n",
      "epoch 737:\n",
      "Loss train 0.0017275948468595743 val 0.15229502320289612\n",
      "epoch 738:\n",
      "Loss train 0.001800381517969072 val 0.1483183056116104\n",
      "epoch 739:\n",
      "Loss train 0.001808668882586062 val 0.15609335899353027\n",
      "epoch 740:\n",
      "Loss train 0.0018249872624874115 val 0.14555944502353668\n",
      "epoch 741:\n",
      "Loss train 0.0017750112684443594 val 0.15358535945415497\n",
      "epoch 742:\n",
      "Loss train 0.0018108684727922081 val 0.1565658003091812\n",
      "epoch 743:\n",
      "Loss train 0.0018337622098624706 val 0.14838166534900665\n",
      "epoch 744:\n",
      "Loss train 0.0018520311843603849 val 0.1500704139471054\n",
      "epoch 745:\n",
      "Loss train 0.0018674632981419564 val 0.15644600987434387\n",
      "epoch 746:\n",
      "Loss train 0.0018152924813330173 val 0.14885704219341278\n",
      "epoch 747:\n",
      "Loss train 0.001761280842125416 val 0.15328910946846008\n",
      "epoch 748:\n",
      "Loss train 0.001842653210274875 val 0.14761760830879211\n",
      "epoch 749:\n",
      "Loss train 0.0017345916191115974 val 0.1508752554655075\n",
      "epoch 750:\n",
      "Loss train 0.0017360245157033204 val 0.1511649340391159\n",
      "epoch 751:\n",
      "Loss train 0.0017546967649832367 val 0.14536814391613007\n",
      "epoch 752:\n",
      "Loss train 0.001843680209480226 val 0.15773114562034607\n",
      "epoch 753:\n",
      "Loss train 0.0018064383827149867 val 0.15148237347602844\n",
      "epoch 754:\n",
      "Loss train 0.001819009612314403 val 0.15014663338661194\n",
      "epoch 755:\n",
      "Loss train 0.0016968177622184158 val 0.14779354631900787\n",
      "epoch 756:\n",
      "Loss train 0.0017565731685608625 val 0.14549557864665985\n",
      "epoch 757:\n",
      "Loss train 0.00179435305390507 val 0.15004917979240417\n",
      "epoch 758:\n",
      "Loss train 0.0017447679378092289 val 0.15786516666412354\n",
      "epoch 759:\n",
      "Loss train 0.0017633201172575354 val 0.15406298637390137\n",
      "epoch 760:\n",
      "Loss train 0.0018524305103346705 val 0.14884166419506073\n",
      "epoch 761:\n",
      "Loss train 0.0017723540710285306 val 0.1521105021238327\n",
      "epoch 762:\n",
      "Loss train 0.0018277602540329098 val 0.1504930704832077\n",
      "epoch 763:\n",
      "Loss train 0.0017658633980900049 val 0.15288211405277252\n",
      "epoch 764:\n",
      "Loss train 0.0018156485464423894 val 0.15046516060829163\n",
      "epoch 765:\n",
      "Loss train 0.0017698543416336178 val 0.15169699490070343\n",
      "epoch 766:\n",
      "Loss train 0.0017681864853948354 val 0.15856869518756866\n",
      "epoch 767:\n",
      "Loss train 0.0018138614818453789 val 0.14570002257823944\n",
      "epoch 768:\n",
      "Loss train 0.0017834524130448698 val 0.15369920432567596\n",
      "epoch 769:\n",
      "Loss train 0.001769851035438478 val 0.15941791236400604\n",
      "epoch 770:\n",
      "Loss train 0.0017199647529050708 val 0.15582787990570068\n",
      "epoch 771:\n",
      "Loss train 0.001786311576142907 val 0.1535467505455017\n",
      "epoch 772:\n",
      "Loss train 0.0017333221267908812 val 0.15360066294670105\n",
      "epoch 773:\n",
      "Loss train 0.0017906953347846867 val 0.15021368861198425\n",
      "epoch 774:\n",
      "Loss train 0.0016983954459428787 val 0.1480562388896942\n",
      "epoch 775:\n",
      "Loss train 0.001781785486266017 val 0.1502745896577835\n",
      "epoch 776:\n",
      "Loss train 0.0017382178772240877 val 0.16190758347511292\n",
      "epoch 777:\n",
      "Loss train 0.0017042501820251346 val 0.15576604008674622\n",
      "epoch 778:\n",
      "Loss train 0.0017974242782220244 val 0.15070940554141998\n",
      "epoch 779:\n",
      "Loss train 0.0017247372521087528 val 0.14946894347667694\n",
      "epoch 780:\n",
      "Loss train 0.0017947593461722137 val 0.1517997831106186\n",
      "epoch 781:\n",
      "Loss train 0.0017322126477956772 val 0.1497006118297577\n",
      "epoch 782:\n",
      "Loss train 0.0017608629176393152 val 0.1515893191099167\n",
      "epoch 783:\n",
      "Loss train 0.001806762395426631 val 0.149362251162529\n",
      "epoch 784:\n",
      "Loss train 0.0017624350292608142 val 0.1599031686782837\n",
      "epoch 785:\n",
      "Loss train 0.00178386920876801 val 0.154103621840477\n",
      "epoch 786:\n",
      "Loss train 0.0017257611872628332 val 0.15558172762393951\n",
      "epoch 787:\n",
      "Loss train 0.0018101769806817174 val 0.14960885047912598\n",
      "epoch 788:\n",
      "Loss train 0.0017479716129601 val 0.14805911481380463\n",
      "epoch 789:\n",
      "Loss train 0.001705571873113513 val 0.15215809643268585\n",
      "epoch 790:\n",
      "Loss train 0.0017573199477046728 val 0.15449175238609314\n",
      "epoch 791:\n",
      "Loss train 0.0016792530156672 val 0.14977489411830902\n",
      "epoch 792:\n",
      "Loss train 0.0017553298575803638 val 0.14940916001796722\n",
      "epoch 793:\n",
      "Loss train 0.001750379068776965 val 0.14734911918640137\n",
      "epoch 794:\n",
      "Loss train 0.0017501812120899558 val 0.15276870131492615\n",
      "epoch 795:\n",
      "Loss train 0.001717295715585351 val 0.15111450850963593\n",
      "epoch 796:\n",
      "Loss train 0.0017798525728285313 val 0.15264631807804108\n",
      "epoch 797:\n",
      "Loss train 0.0017401988441124558 val 0.1553998589515686\n",
      "epoch 798:\n",
      "Loss train 0.0017929834676906466 val 0.1511966586112976\n",
      "epoch 799:\n",
      "Loss train 0.0016866577491164208 val 0.15456755459308624\n",
      "epoch 800:\n",
      "Loss train 0.0017068967521190644 val 0.15021613240242004\n",
      "epoch 801:\n",
      "Loss train 0.0016321210097521544 val 0.1526767611503601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 802:\n",
      "Loss train 0.0017339678881689907 val 0.15124759078025818\n",
      "epoch 803:\n",
      "Loss train 0.001695038340985775 val 0.1509517878293991\n",
      "epoch 804:\n",
      "Loss train 0.0016593516115099192 val 0.1624843180179596\n",
      "epoch 805:\n",
      "Loss train 0.0016832643570378423 val 0.15924693644046783\n",
      "epoch 806:\n",
      "Loss train 0.0017579307118430734 val 0.15793012082576752\n",
      "epoch 807:\n",
      "Loss train 0.001722444494254887 val 0.15176518261432648\n",
      "epoch 808:\n",
      "Loss train 0.0017574969111010432 val 0.15235337615013123\n",
      "epoch 809:\n",
      "Loss train 0.0017059246180579067 val 0.15495409071445465\n",
      "epoch 810:\n",
      "Loss train 0.0018163103740662337 val 0.15893042087554932\n",
      "epoch 811:\n",
      "Loss train 0.0017268818197771908 val 0.15285667777061462\n",
      "epoch 812:\n",
      "Loss train 0.0017896259939298035 val 0.15540799498558044\n",
      "epoch 813:\n",
      "Loss train 0.0016998629653826355 val 0.1537809520959854\n",
      "epoch 814:\n",
      "Loss train 0.0016776089705526828 val 0.156813845038414\n",
      "epoch 815:\n",
      "Loss train 0.001722911319695413 val 0.15452471375465393\n",
      "epoch 816:\n",
      "Loss train 0.0017661166358739137 val 0.15477928519248962\n",
      "epoch 817:\n",
      "Loss train 0.0017564544342458248 val 0.15721145272254944\n",
      "epoch 818:\n",
      "Loss train 0.0017126328069716691 val 0.16105371713638306\n",
      "epoch 819:\n",
      "Loss train 0.0016821490535512567 val 0.15663130581378937\n",
      "epoch 820:\n",
      "Loss train 0.0016597891421988606 val 0.15232528746128082\n",
      "epoch 821:\n",
      "Loss train 0.0017443334693089127 val 0.14921213686466217\n",
      "epoch 822:\n",
      "Loss train 0.0017600180627778171 val 0.15187481045722961\n",
      "epoch 823:\n",
      "Loss train 0.0016476253801956772 val 0.15217849612236023\n",
      "epoch 824:\n",
      "Loss train 0.0017261875439435245 val 0.1460988074541092\n",
      "epoch 825:\n",
      "Loss train 0.001691035552881658 val 0.15509867668151855\n",
      "epoch 826:\n",
      "Loss train 0.001717550271190703 val 0.1481209695339203\n",
      "epoch 827:\n",
      "Loss train 0.0016667428575456143 val 0.15306437015533447\n",
      "epoch 828:\n",
      "Loss train 0.0016782915685325862 val 0.15184195339679718\n",
      "epoch 829:\n",
      "Loss train 0.001666098059155047 val 0.15457041561603546\n",
      "epoch 830:\n",
      "Loss train 0.001681085449643433 val 0.15258285403251648\n",
      "epoch 831:\n",
      "Loss train 0.0017234518332406878 val 0.15401634573936462\n",
      "epoch 832:\n",
      "Loss train 0.0016891851145774126 val 0.15484653413295746\n",
      "epoch 833:\n",
      "Loss train 0.0017545185005292297 val 0.15788178145885468\n",
      "epoch 834:\n",
      "Loss train 0.0017154041426256299 val 0.1576111763715744\n",
      "epoch 835:\n",
      "Loss train 0.0017197985742241145 val 0.15078112483024597\n",
      "epoch 836:\n",
      "Loss train 0.0016348410677164793 val 0.15049995481967926\n",
      "epoch 837:\n",
      "Loss train 0.0016347845988348127 val 0.1577654480934143\n",
      "epoch 838:\n",
      "Loss train 0.0016594633320346475 val 0.15709185600280762\n",
      "epoch 839:\n",
      "Loss train 0.0016602054061368107 val 0.15834330022335052\n",
      "epoch 840:\n",
      "Loss train 0.0017235163180157542 val 0.15925799310207367\n",
      "epoch 841:\n",
      "Loss train 0.001684918194077909 val 0.15741094946861267\n",
      "epoch 842:\n",
      "Loss train 0.0016765532977879047 val 0.15546226501464844\n",
      "epoch 843:\n",
      "Loss train 0.0016855812966823577 val 0.15077278017997742\n",
      "epoch 844:\n",
      "Loss train 0.0017105631185695528 val 0.14742344617843628\n",
      "epoch 845:\n",
      "Loss train 0.0016623738687485457 val 0.14468444883823395\n",
      "epoch 846:\n",
      "Loss train 0.0017203925158828497 val 0.15639638900756836\n",
      "epoch 847:\n",
      "Loss train 0.0017001742403954267 val 0.14591564238071442\n",
      "epoch 848:\n",
      "Loss train 0.0016102796457707882 val 0.15488962829113007\n",
      "epoch 849:\n",
      "Loss train 0.0016950723528862 val 0.15754498541355133\n",
      "epoch 850:\n",
      "Loss train 0.001766066805459559 val 0.15430030226707458\n",
      "epoch 851:\n",
      "Loss train 0.0017225926499813795 val 0.15494446456432343\n",
      "epoch 852:\n",
      "Loss train 0.0016589640118181705 val 0.1492629200220108\n",
      "epoch 853:\n",
      "Loss train 0.0016252814875915647 val 0.15584926307201385\n",
      "epoch 854:\n",
      "Loss train 0.001704072174616158 val 0.15971575677394867\n",
      "epoch 855:\n",
      "Loss train 0.0016587710287421942 val 0.1560562252998352\n",
      "epoch 856:\n",
      "Loss train 0.0017025665547698737 val 0.15511488914489746\n",
      "epoch 857:\n",
      "Loss train 0.0016494522420689463 val 0.15121863782405853\n",
      "epoch 858:\n",
      "Loss train 0.0016199685866013168 val 0.16188392043113708\n",
      "epoch 859:\n",
      "Loss train 0.0016617729188874364 val 0.14985717833042145\n",
      "epoch 860:\n",
      "Loss train 0.00169380066357553 val 0.15381944179534912\n",
      "epoch 861:\n",
      "Loss train 0.0016779706794768571 val 0.16076277196407318\n",
      "epoch 862:\n",
      "Loss train 0.0016713179387152196 val 0.15192197263240814\n",
      "epoch 863:\n",
      "Loss train 0.0017182037327438593 val 0.150797039270401\n",
      "epoch 864:\n",
      "Loss train 0.001673705218359828 val 0.14557960629463196\n",
      "epoch 865:\n",
      "Loss train 0.0017143960632383824 val 0.1555860936641693\n",
      "epoch 866:\n",
      "Loss train 0.0016470992881804705 val 0.15846727788448334\n",
      "epoch 867:\n",
      "Loss train 0.0016744649782776833 val 0.15344977378845215\n",
      "epoch 868:\n",
      "Loss train 0.0016488830763846636 val 0.15913383662700653\n",
      "epoch 869:\n",
      "Loss train 0.0016725956033915282 val 0.1518389731645584\n",
      "epoch 870:\n",
      "Loss train 0.0016872216034680605 val 0.15231281518936157\n",
      "epoch 871:\n",
      "Loss train 0.0016459057852625846 val 0.15875102579593658\n",
      "epoch 872:\n",
      "Loss train 0.0016232526106759906 val 0.15126225352287292\n",
      "epoch 873:\n",
      "Loss train 0.0016563377911224962 val 0.15055622160434723\n",
      "epoch 874:\n",
      "Loss train 0.0016088188961148261 val 0.15462495386600494\n",
      "epoch 875:\n",
      "Loss train 0.0016676255147904159 val 0.15307460725307465\n",
      "epoch 876:\n",
      "Loss train 0.0016337593253701926 val 0.14681373536586761\n",
      "epoch 877:\n",
      "Loss train 0.0016896924348548054 val 0.15297062695026398\n",
      "epoch 878:\n",
      "Loss train 0.0016539642801508307 val 0.15348494052886963\n",
      "epoch 879:\n",
      "Loss train 0.0016403272263705731 val 0.14397026598453522\n",
      "epoch 880:\n",
      "Loss train 0.0016332492623478173 val 0.14890380203723907\n",
      "epoch 881:\n",
      "Loss train 0.0016406444692984223 val 0.15846556425094604\n",
      "epoch 882:\n",
      "Loss train 0.0015879103038460016 val 0.161289781332016\n",
      "epoch 883:\n",
      "Loss train 0.001627413728274405 val 0.151991605758667\n",
      "epoch 884:\n",
      "Loss train 0.0016161528127267956 val 0.1519332081079483\n",
      "epoch 885:\n",
      "Loss train 0.0016628380483016372 val 0.16037526726722717\n",
      "epoch 886:\n",
      "Loss train 0.0016257566027343274 val 0.1579645276069641\n",
      "epoch 887:\n",
      "Loss train 0.0016470879791304468 val 0.15070010721683502\n",
      "epoch 888:\n",
      "Loss train 0.0016507543912157416 val 0.1545991450548172\n",
      "epoch 889:\n",
      "Loss train 0.0015860718917101622 val 0.15473173558712006\n",
      "epoch 890:\n",
      "Loss train 0.0015619801292195915 val 0.163887158036232\n",
      "epoch 891:\n",
      "Loss train 0.0016769485706463457 val 0.15457788109779358\n",
      "epoch 892:\n",
      "Loss train 0.0016099313711747528 val 0.15087400376796722\n",
      "epoch 893:\n",
      "Loss train 0.001653737393207848 val 0.14904412627220154\n",
      "epoch 894:\n",
      "Loss train 0.0015945094944909215 val 0.153105691075325\n",
      "epoch 895:\n",
      "Loss train 0.0016302852164953947 val 0.14385180175304413\n",
      "epoch 896:\n",
      "Loss train 0.00163982427213341 val 0.1548875868320465\n",
      "epoch 897:\n",
      "Loss train 0.001670543560758233 val 0.15400053560733795\n",
      "epoch 898:\n",
      "Loss train 0.0016207637600600718 val 0.14990344643592834\n",
      "epoch 899:\n",
      "Loss train 0.0016180981863290072 val 0.15894976258277893\n",
      "epoch 900:\n",
      "Loss train 0.0015770996771752835 val 0.15779191255569458\n",
      "epoch 901:\n",
      "Loss train 0.0016500348448753358 val 0.15107297897338867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 902:\n",
      "Loss train 0.0015547755807638168 val 0.1505279839038849\n",
      "epoch 903:\n",
      "Loss train 0.0016641313079744577 val 0.15718239545822144\n",
      "epoch 904:\n",
      "Loss train 0.0016751339565962552 val 0.15656980872154236\n",
      "epoch 905:\n",
      "Loss train 0.001621507448144257 val 0.14594388008117676\n",
      "epoch 906:\n",
      "Loss train 0.001595003355294466 val 0.14666451513767242\n",
      "epoch 907:\n",
      "Loss train 0.0016635555867105722 val 0.152962788939476\n",
      "epoch 908:\n",
      "Loss train 0.0015704177422448992 val 0.15788963437080383\n",
      "epoch 909:\n",
      "Loss train 0.001634200258180499 val 0.15193381905555725\n",
      "epoch 910:\n",
      "Loss train 0.0015874839220196009 val 0.15949764847755432\n",
      "epoch 911:\n",
      "Loss train 0.001571672074496746 val 0.1528662145137787\n",
      "epoch 912:\n",
      "Loss train 0.0016352103427052498 val 0.15462033450603485\n",
      "epoch 913:\n",
      "Loss train 0.0016193594196811318 val 0.15323230624198914\n",
      "epoch 914:\n",
      "Loss train 0.0015919493576511741 val 0.15612585842609406\n",
      "epoch 915:\n",
      "Loss train 0.0015674020079895854 val 0.1542551964521408\n",
      "epoch 916:\n",
      "Loss train 0.001662036132067442 val 0.15779165923595428\n",
      "epoch 917:\n",
      "Loss train 0.0017044400544837118 val 0.15671570599079132\n",
      "epoch 918:\n",
      "Loss train 0.0016086421646177768 val 0.16200393438339233\n",
      "epoch 919:\n",
      "Loss train 0.0016324185859411956 val 0.15587812662124634\n",
      "epoch 920:\n",
      "Loss train 0.001640601709485054 val 0.1531437188386917\n",
      "epoch 921:\n",
      "Loss train 0.0015622769789770245 val 0.17044247686862946\n",
      "epoch 922:\n",
      "Loss train 0.0015991973504424096 val 0.1570606678724289\n",
      "epoch 923:\n",
      "Loss train 0.0016387889133766293 val 0.16076859831809998\n",
      "epoch 924:\n",
      "Loss train 0.0015595776764675976 val 0.16158312559127808\n",
      "epoch 925:\n",
      "Loss train 0.0015761225335299968 val 0.15844380855560303\n",
      "epoch 926:\n",
      "Loss train 0.001607925321906805 val 0.14969293773174286\n",
      "epoch 927:\n",
      "Loss train 0.0016699529811739923 val 0.1507733166217804\n",
      "epoch 928:\n",
      "Loss train 0.0015922723729163409 val 0.15503546595573425\n",
      "epoch 929:\n",
      "Loss train 0.0016298718210309743 val 0.15768186748027802\n",
      "epoch 930:\n",
      "Loss train 0.0016035170154646038 val 0.15708720684051514\n",
      "epoch 931:\n",
      "Loss train 0.0015787400007247925 val 0.14649198949337006\n",
      "epoch 932:\n",
      "Loss train 0.0015925043476745487 val 0.15565082430839539\n",
      "epoch 933:\n",
      "Loss train 0.0015772198969498277 val 0.15183386206626892\n",
      "epoch 934:\n",
      "Loss train 0.001597255221568048 val 0.1634989231824875\n",
      "epoch 935:\n",
      "Loss train 0.00160759721044451 val 0.15071210265159607\n",
      "epoch 936:\n",
      "Loss train 0.0016933042956516146 val 0.15204595029354095\n",
      "epoch 937:\n",
      "Loss train 0.0015795262213796376 val 0.15877461433410645\n",
      "epoch 938:\n",
      "Loss train 0.0015681220991536975 val 0.1544090360403061\n",
      "epoch 939:\n",
      "Loss train 0.0015424240883439779 val 0.15231168270111084\n",
      "epoch 940:\n",
      "Loss train 0.0015327406469732522 val 0.1622539609670639\n",
      "epoch 941:\n",
      "Loss train 0.0015761948842555286 val 0.15505485236644745\n",
      "epoch 942:\n",
      "Loss train 0.0016397590367123484 val 0.16223344206809998\n",
      "epoch 943:\n",
      "Loss train 0.0015942914318293332 val 0.15361981093883514\n",
      "epoch 944:\n",
      "Loss train 0.0015833706706762313 val 0.15367178618907928\n",
      "epoch 945:\n",
      "Loss train 0.001655883500352502 val 0.15679751336574554\n",
      "epoch 946:\n",
      "Loss train 0.0015988170178607105 val 0.15316715836524963\n",
      "epoch 947:\n",
      "Loss train 0.0015276567498221994 val 0.15208961069583893\n",
      "epoch 948:\n",
      "Loss train 0.0016271825833246113 val 0.15273281931877136\n",
      "epoch 949:\n",
      "Loss train 0.0016510631665587424 val 0.15094776451587677\n",
      "epoch 950:\n",
      "Loss train 0.0016387157551944257 val 0.15335948765277863\n",
      "epoch 951:\n",
      "Loss train 0.0015663921246305108 val 0.152971550822258\n",
      "epoch 952:\n",
      "Loss train 0.001593816762790084 val 0.1580934077501297\n",
      "epoch 953:\n",
      "Loss train 0.0015643848478794097 val 0.16012027859687805\n",
      "epoch 954:\n",
      "Loss train 0.0015795289259403944 val 0.15927979350090027\n",
      "epoch 955:\n",
      "Loss train 0.0015626673037186264 val 0.1555158644914627\n",
      "epoch 956:\n",
      "Loss train 0.001587837740778923 val 0.15658393502235413\n",
      "epoch 957:\n",
      "Loss train 0.0015751230455935002 val 0.15245689451694489\n",
      "epoch 958:\n",
      "Loss train 0.0016073012743145227 val 0.1527339369058609\n",
      "epoch 959:\n",
      "Loss train 0.0015261926660314203 val 0.15912705659866333\n",
      "epoch 960:\n",
      "Loss train 0.0015691249296069145 val 0.15460793673992157\n",
      "epoch 961:\n",
      "Loss train 0.00156034473516047 val 0.15570785105228424\n",
      "epoch 962:\n",
      "Loss train 0.0015481521710753441 val 0.15928417444229126\n",
      "epoch 963:\n",
      "Loss train 0.0015639919992536306 val 0.15408244729042053\n",
      "epoch 964:\n",
      "Loss train 0.0015288413213565945 val 0.1587395817041397\n",
      "epoch 965:\n",
      "Loss train 0.0015552104022353887 val 0.15449677407741547\n",
      "epoch 966:\n",
      "Loss train 0.0015920441076159478 val 0.1641467809677124\n",
      "epoch 967:\n",
      "Loss train 0.0015898642437532544 val 0.15981952846050262\n",
      "epoch 968:\n",
      "Loss train 0.00158332194481045 val 0.15553231537342072\n",
      "epoch 969:\n",
      "Loss train 0.0015657851481810211 val 0.15207019448280334\n",
      "epoch 970:\n",
      "Loss train 0.00164001290127635 val 0.14940495789051056\n",
      "epoch 971:\n",
      "Loss train 0.0015842305477708578 val 0.1571180373430252\n",
      "epoch 972:\n",
      "Loss train 0.0015470805438235402 val 0.14980295300483704\n",
      "epoch 973:\n",
      "Loss train 0.0015768927233293652 val 0.152791827917099\n",
      "epoch 974:\n",
      "Loss train 0.0015498677790164947 val 0.15772227942943573\n",
      "epoch 975:\n",
      "Loss train 0.0015387787772342563 val 0.15636539459228516\n",
      "epoch 976:\n",
      "Loss train 0.001564308736473322 val 0.15203562378883362\n",
      "epoch 977:\n",
      "Loss train 0.001579328096471727 val 0.1560751348733902\n",
      "epoch 978:\n",
      "Loss train 0.0015183023903518915 val 0.16061660647392273\n",
      "epoch 979:\n",
      "Loss train 0.001539833231829107 val 0.1544445902109146\n",
      "epoch 980:\n",
      "Loss train 0.0015352427884936333 val 0.1596454679965973\n",
      "epoch 981:\n",
      "Loss train 0.0015369599256664515 val 0.15651006996631622\n",
      "epoch 982:\n",
      "Loss train 0.0015086328368633987 val 0.15847575664520264\n",
      "epoch 983:\n",
      "Loss train 0.0015095124244689942 val 0.1532902717590332\n",
      "epoch 984:\n",
      "Loss train 0.0015774557972326874 val 0.15469825267791748\n",
      "epoch 985:\n",
      "Loss train 0.0016057667843997479 val 0.1589973419904709\n",
      "epoch 986:\n",
      "Loss train 0.0015050533572211861 val 0.14857959747314453\n",
      "epoch 987:\n",
      "Loss train 0.0015375357912853361 val 0.1583719402551651\n",
      "epoch 988:\n",
      "Loss train 0.0015398084251210093 val 0.15674450993537903\n",
      "epoch 989:\n",
      "Loss train 0.0015240723136812448 val 0.15603581070899963\n",
      "epoch 990:\n",
      "Loss train 0.0015611802060157061 val 0.1602839231491089\n",
      "epoch 991:\n",
      "Loss train 0.0014952396396547556 val 0.1576736569404602\n",
      "epoch 992:\n",
      "Loss train 0.0015835567880421878 val 0.15994793176651\n",
      "epoch 993:\n",
      "Loss train 0.0015158658884465695 val 0.16234466433525085\n",
      "epoch 994:\n",
      "Loss train 0.0015066392859444022 val 0.1508636474609375\n",
      "epoch 995:\n",
      "Loss train 0.001559368684887886 val 0.16031768918037415\n",
      "epoch 996:\n",
      "Loss train 0.0015141592323780059 val 0.151407390832901\n",
      "epoch 997:\n",
      "Loss train 0.0015570515478029847 val 0.16349107027053833\n",
      "epoch 998:\n",
      "Loss train 0.001538005093112588 val 0.16172678768634796\n",
      "epoch 999:\n",
      "Loss train 0.001498372633010149 val 0.16538919508457184\n",
      "epoch 1000:\n",
      "Loss train 0.0016060819756239652 val 0.15574686229228973\n",
      "epoch 1001:\n",
      "Loss train 0.001521105144172907 val 0.15805725753307343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:41<00:00, 24.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1002:\n",
      "Loss train 0.0015119687505066395 val 0.16629712283611298\n",
      "epoch 1003:\n",
      "Loss train 0.0015930710565298797 val 0.15521475672721863\n",
      "epoch 1004:\n",
      "Loss train 0.0015370525941252708 val 0.16002029180526733\n",
      "epoch 1005:\n",
      "Loss train 0.0014674914488568903 val 0.1596078723669052\n",
      "epoch 1006:\n",
      "Loss train 0.0015139001673087477 val 0.16359573602676392\n",
      "epoch 1007:\n",
      "Loss train 0.0014771541142836212 val 0.16047263145446777\n",
      "epoch 1008:\n",
      "Loss train 0.0015784609531983734 val 0.1667172610759735\n",
      "epoch 1009:\n",
      "Loss train 0.0015269925305619837 val 0.15235790610313416\n",
      "epoch 1010:\n",
      "Loss train 0.0015519520444795488 val 0.1600360870361328\n",
      "epoch 1011:\n",
      "Loss train 0.001533260243013501 val 0.154218852519989\n",
      "epoch 1012:\n",
      "Loss train 0.0015023225713521242 val 0.1583917886018753\n",
      "epoch 1013:\n",
      "Loss train 0.0015292229345068335 val 0.15360954403877258\n",
      "epoch 1014:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     15\u001b[0m ema\u001b[38;5;241m.\u001b[39mapply_shadow()\n",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m ddpm(data, cond_data)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ################# DEBUG ########################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# ################# DEBUG ########################\u001b[39;00m\n\u001b[0;32m     37\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arne\\anaconda3\\envs\\hf_diff\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "best_val_loss = 1_000_000.\n",
    "save_dir =  os.path.join(logging_dir, \"weights/\")\n",
    "\n",
    "ema = EMA(model, decay=decay_rate)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    print('epoch {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    ema.apply_shadow()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (val_data, val_cond) in enumerate(val_loader):\n",
    "            val_data = val_data.to(device)\n",
    "        \n",
    "            val_cond = val_cond.float()\n",
    "            val_cond = val_cond.to(device)\n",
    "            val_loss = ddpm(val_data, val_cond)\n",
    "            \n",
    "            running_val_loss += val_loss\n",
    "    \n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    print('Loss train {} val {}'.format(avg_loss, avg_val_loss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_val_loss },\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    writer.flush()\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_dict = model.state_dict()\n",
    "        best_model_opt = optim.state_dict()\n",
    "        best_model_shadow = ema.shadow\n",
    "        \n",
    "    \n",
    "    if epoch_number % save_rate == 0:\n",
    "        generated_sample = ddpm.sample(batch_size, real_cond_data_val.to(device))\n",
    "        generated_sample = generated_sample.cpu().numpy()\n",
    "        \n",
    "        plot_kde_samples(generated_sample, real_data_val,show=False, fpath=os.path.join(logging_dir, \"kde/\", f\"kde_epoch_{epoch_number}.png\"), epoch=epoch_number)\n",
    "        \n",
    "        model_name = f\"model_epoch_{epoch_number}_val_{avg_val_loss:.4f}.pth\"\n",
    "        save_model(model.state_dict(), ema.shadow, optim.state_dict(), os.path.join(save_dir, model_name))\n",
    "    \n",
    "    ema.restore()\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_model_dict, best_model_shadow, best_model_opt, os.path.join(logging_dir, \"weights/\", \"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbecf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, ema, optim, os.path.join(logging_dir, \"weights/\", \"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f37315",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for kde_plot in os.listdir(os.path.join(logging_dir, \"kde/\")):\n",
    "    paths.append(os.path.join(logging_dir, \"kde/\", kde_plot))\n",
    "\n",
    "make_gif_from_images(paths, os.path.join(logging_dir, \"kde/\", \"kde_progression.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16046b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffa8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_test, real_cond_data_test = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee051c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = ddpm.sample(batch_size, real_cond_data_test.to(device))\n",
    "    samples = samples.cpu().numpy()\n",
    "\n",
    "print(f\"Samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizual_comparison(samples, real_data_test, os.path.join(logging_dir, \"viz/\", \"pca_umap_tsne_all_batches.png\"), use_all_data=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizual_comparison(samples, real_data_test, os.path.join(logging_dir, \"viz/\", \"pca_umap_tsne_per_batch.png\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_histogram_per_customer(samples, real_data_test, fpath=os.path.join(logging_dir, \"mmd/\", \"mmd.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jsd_per_customer(samples, real_data_test, os.path.join(logging_dir, \"jsd/\", \"jsd.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a40031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde_samples(samples, real_data_test, fpath=os.path.join(logging_dir, \"kde/\", f\"kde.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f370bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = np.random.randint(0, 256) \n",
    "customer_indices = np.random.randint(0, 15, size=15)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample = ddpm.sample(1, real_cond_data_test.to(device)) \n",
    "    sample = sample.cpu().numpy() \n",
    "\n",
    "for i in customer_indices:\n",
    "    axs[0].plot(sample[0, i], alpha=0.7, label=f'Customer {i}')\n",
    "    axs[1].plot(real_data_test[0, i], alpha=0.7, label=f'Customer {i}')\n",
    "\n",
    "axs[0].set_title(\"Generated Data\")\n",
    "axs[1].set_title(\"Real Data\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(logging_dir, \"ts_sample/\", \"samples.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a817ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
